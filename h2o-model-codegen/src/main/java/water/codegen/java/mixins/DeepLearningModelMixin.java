package water.codegen.java.mixins;

import hex.genmodel.annotations.CG;

/**
 * Created by michal on 3/28/16.
 */
public class DeepLearningModelMixin extends ModelMixin {

  // Published
  @CG(delegate = CG.NA, comment = "Thread-local storage for input neuron activation values.")
  public final double[] NUMS;

  @CG(delegate = "#model_info#data_info._normMul", comment = "Standardization/Normalization scaling factor for numerical variables.")
  public static final double[] NORMMUL = null;

  @CG(delegate = "#model_info#data_info._normSub", comment = "Standardization/Normalization offset for numerical variables.")
  public static final double[] NORMSUB = null;

  @CG(delegate = "#model_info#data_info._cats", comment = "Workspace for storing categorical input variables.")
  public static final int[] CATS;

  @CG(delegate = "#model_info#data_info._catOffsets", comment = "Workspace for categorical offsets.")
  public static final int[] CATOFFSET = null;

  @CG(delegate = "#model_info#data_info._normRespMul", comment = "Standardization/Normalization scaling factor for response.")
  public static final double[] NORMRESPMUL = null;

  @CG(delegate = "#model_info#data_info._normRespSub", comment = "Standardization/Normalization offset for response.")
  public static final double[] NORMRESPSUB = null;

  @CG(delegate = "#model_info#get_params._hidden_dropout_ratios", comment = "Hidden layer dropout ratios.")
  public static final double[] HIDDEN_DROPOUT_RATIOS = null;

  @CG(delegate = CG.NA, comment = "Number of neurons for each layer.")
  public static final int[] NEURONS = XXX;

  // FIXME
  public static final float[][] WEIGHT = XXX;
  // FIXME
  public static final double[][] BIAS = XXX;


  /**
   * AutoGenerated properties
   */
  @CG(delegate = "#model_info#data_info._cats")
  private static final int GEN_CATS = -1;

  @CG(delegate = "#model_info#data_info._nums")
  private static final int GEN_NUMS = -1;

  @CG(delegate = "#model_info#data_info._useAllFactorLevels")
  private static final boolean GEN_USE_ALL_FACTOR_LEVELS = false;

  @CG(delegate = "#model_info#get_params._activation#isTanh")
  private static final boolean GEN_IS_TANH = false;

  @CG(delegate = "#model_info#get_params._activation#isRelu")
  private static final boolean GEN_IS_RELU = false;

  @CG(delegate = "#model_info#get_params._activation#isMaxout")
  private static final boolean GEN_IS_MAXOUT = false;

  @CG(delegate = "#model_info#get_params._autoencoder")
  private static final boolean GEN_IS_AUTOENCODER = false;

  @CG(delegate = "#model_info#data_info#numStart")
  private static final int GEN_NUMSTART = -1;

  @CG(delegate = "#model_info#data_info#fullN")
  private static final int GEN_FULLN = -1;

  // --- COPY --

  // Static initialization
  static {
    CATS = GEN_CATS > 0 ? new int[GEN_CATS] : null;
  }

  // Instance initialization
  {
    NUMS = GEN_NUMS > 0 ? new double[GEN_NUMS] : null;
  }


  public static void score0(double[] data, double[] preds, double[] nums, int[] cats /* FIXME: cats?? */, double[][] activation) {
    java.util.Arrays.fill(preds,0);
    java.util.Arrays.fill(activation[0],0);
    int i = 0, ncats = 0;
    if (GEN_CATS > 0) {
      java.util.Arrays.fill(cats, 0);
      for (; i < GEN_CATS; i++) {
        if (!Double.isNaN(data[i])) {
          int c = (int) data[i];
          if (GEN_USE_ALL_FACTOR_LEVELS) {
            cats[ncats++] = c + CATOFFSET[i];
          } else {
            if (c != 0) cats[ncats++] = c + CATOFFSET[i] -1;
          }
        }
      }
      for (i=0; i<ncats; ++i) activation[0][cats[i]] = 1;
    }
    if (GEN_NUMS > 0) {
      java.util.Arrays.fill(nums, 0);
      final int n = data.length; // FIXME: data.lenght is known before, can be generated
      final int cats_off = GEN_CATS > 0 ? - GEN_CATS : 0;
      for (; i < n; i++) {
        nums[i + cats_off] = Double.isNaN(data[i]) ? 0 : // FIXME +cats_off can be replace by - GEN_CATS
                             NORMMUL != null ? (data[i] - NORMSUB[i + cats_off] * NORMMUL[i + cats_off])
                                             : data[i];
      }
      for (i=0; i<nums.length; ++i) {
        activation[0][CATOFFSET[CATOFFSET.length-1] + i] = Double.isNaN(nums[i]) ? 0 : nums[i];
      }
    }
    final int stopping = GEN_IS_AUTOENCODER ? activation.length : activation.length - 1;

    // make prediction: forward propagation
    for (i = 1; i < activation.length; ++i) {
      java.util.Arrays.fill(activation[i],0);
      if (GEN_IS_MAXOUT) {
        int _k = 2; // channels
        if (i < stopping) {
          double[] channel = new double[_k];
          for (int r=0; r<activation[i].length; ++r) {
            final int cols = activation[i-1].length;
            short maxK = 0;
            for (short k = 0; k < _k; ++k) {
              channel[k] = 0;
              for (int c=0; c<cols; ++c) {
                channel[k] += WEIGHT[i][_k*(r * cols + c) + k] * activation[i-1][c];
              }
              channel[k] += BIAS[i][_k*r+k];
              if (channel[k] > channel[maxK]) maxK=k;
            }
            activation[i][r] = channel[maxK];
            if (HIDDEN_DROPOUT_RATIOS != null) {
              if (i < activation.length-1) {
                activation[i][r] *= HIDDEN_DROPOUT_RATIOS[i-1];
              }
            }
          }
        }
        if (i == activation.length-1) {
         // FIXME: pure mathvec
        }
      } else { // else is not maxout
        // pure matvec
        // FIXME here pure matvec
        if (i < stopping) {
          for (int r=0; r<activation[i].length; ++r) {
            if (GEN_IS_TANH) {
              activation[i][r] = 1 - 2 / (1 + Math.exp(2*activation[i][r]));
            } else if (GEN_IS_RELU) {
              activation[i][r] = Math.max(0, activation[i][r]);
            }
            if (HIDDEN_DROPOUT_RATIOS != null) {
              if (i < activation.length-1) {
                activation[i][r] *= HIDDEN_DROPOUT_RATIOS[i-1];
              }
            }
          } // 2
        }  //1
      }
      if (GEN_IS_CLASSIFIER) {
        if (i == activation.length-1) {
          double max = activation[i][0];
          for (int r=1; r<activation[i].length; r++) {
            if (activation[i][r]>max) max = activation[i][r];
          }
          double scale = 0;
          for (int r=0; r<activation[i].length; r++) {
            activation[i][r] = Math.exp(activation[i][r] - max);
            scale += activation[i][r];
          }
          for (int r=0; r<activation[i].length; r++) {
            if (Double.isNaN(activation[i][r])) throw new RuntimeException("Numerical instability, predicted NaN.");
            activation[i][r] /= scale;
            preds[r+1] = activation[i][r];
          }
        }
      } else if (!GEN_IS_AUTOENCODER) {
        if (i == activation.length-1) {
          if (NORMRESPMUL != null) {
            preds[1] = (activation[i][0] / NORMRESPMUL[0] + NORMRESPSUB[0]);
          } else {
            preds[1] = activation[i][0];
          }
          preds[1] = linkInv(preds[1]);
          if (Double.isNaN(preds[1])) throw new RuntimeException("Predicted regression target NaN!");
        }
      } else { // is autoencoder
        if (i == activation.length-1) {
          for (int r=0; r<activation[i].length; r++) {
            if (Double.isNaN(activation[i][r])) throw new RuntimeException("Numerical instability, reconstructed NaN.");
            preds[r] = activation[i][r];
          }
        }
        if (GEN_NUMS > 0) {
          for(int k=GEN_NUMSTART; k < GEN_FULLN; k++) {
            preds[k] = preds[k] / NORMMUL[k-GEN_NUMSTART] + NORMSUB[k-GEN_NUMSTART];
          }
        }
      }
    }
    if (GEN_IS_AUTOENCODER) {
      return;
    } else if (GEN_IS_CLASSIFIER) {
      if (GEN_BALANCE_CLASSES) {
        hex.genmodel.GenModel.correctProbabilities(preds, PRIOR_CLASS_DISTRIB, MODEL_CLASS_DISTRIB);
        preds[0] = hex.genmodel.GenModel.getPrediction(preds, PRIOR_CLASS_DISTRIB, data, DEFAULT_THRESHOLD);
      }
    } else {
      preds[0] = preds[1];
    }
  }

  final static void pureMatVec(double[][] activation, int i) {
    int cols = activation[i-1].length;
    int rows = activation[i].length;
    int extra=cols-cols%8;
    int multiple = (cols/8)*8-1;

    int idx = 0;
    float[] a = WEIGHT[i];
    double[] x = activation[i-1];
    double[] y = BIAS[i];
    double[] res = activation[i];
    for (int row=0; row<rows; ++row) {
      double psum0 = 0, psum1 = 0, psum2 = 0, psum3 = 0, psum4 = 0, psum5 = 0, psum6 = 0, psum7 = 0;
      for (int col = 0; col < multiple; col += 8) {
        int off = idx + col;
        psum0 += a[off    ] * x[col    ];
        psum1 += a[off + 1] * x[col + 1];
        psum2 += a[off + 2] * x[col + 2];
        psum3 += a[off + 3] * x[col + 3];
        psum4 += a[off + 4] * x[col + 4];
        psum5 += a[off + 5] * x[col + 5];
        psum6 += a[off + 6] * x[col + 6];
        psum7 += a[off + 7] * x[col + 7];
      }
      res[row] += psum0 + psum1 + psum2 + psum3;
      res[row] += psum4 + psum5 + psum6 + psum7;
      for (int col = extra; col < cols; col++)
        res[row] += a[idx + col] * x[col];
      res[row] += y[row];
      idx += cols;
    }
  }
  // --- END ---
}
