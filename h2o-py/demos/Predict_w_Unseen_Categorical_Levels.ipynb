{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What Happens When You Predict on an Unseen Categorical Level\n",
    "i.e. a rare event appears in your test set, which wasn't present in your training set.\n",
    "___\n",
    "\n",
    "> *This notebook walks through the steps of importing, cleaning, training, and testing on a data set where the test set contains a categorical level that was not present in the training set. You need to run steps 2. - 6. (to load all the variables) before you can jump between sections and run individual cells*\n",
    "\n",
    "> Up to date for release H2O cluster version: 3.8.2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "0. [Jump to The Short Answer](#question)\n",
    "1. [Import, Visualize & Clean Data](#start)\n",
    "2. [Split into Train and Test Sets](#split) \n",
    "3. [Build Models for GLM, GBM, DRF, Deep Learning & Naive Bayes](#build)\n",
    "4. [Model Performance for GLM, GBM, DRF, Deep Learning & Naive Bayes](#output)\n",
    "5. [How Each Algorithm Handles New Categorical Levels During Prediction](#answer)\n",
    "6. [Bonus Section! How Each Algorithm Handles Missing Values During Training & Testing?](#missing)\n",
    "6. [Additional Resources](#resources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"start\"></a>\n",
    "## Import Packages, Initialize an H2O Cluster & Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h2o, pandas, pprint, operator, numpy as np, matplotlib.pyplot as plt\n",
    "from h2o.estimators.glm import H2OGeneralizedLinearEstimator\n",
    "from h2o.estimators.gbm import H2OGradientBoostingEstimator\n",
    "from h2o.estimators.random_forest import H2ORandomForestEstimator\n",
    "from h2o.estimators.deeplearning import H2ODeepLearningEstimator\n",
    "from h2o.estimators.naive_bayes import H2ONaiveBayesEstimator\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set 'interactive = True' for interactive plots, 'interactive = False' if not:\n",
    "interactive = True\n",
    "if not interactive: matplotlib.use('Agg', warn=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are you sure you want to shutdown the H2O instance running at localhost:54321 (Y/N)? Y\n"
     ]
    }
   ],
   "source": [
    "h2o.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "No instance found at ip and port: localhost:54321. Trying to start local jar...\n",
      "\n",
      "\n",
      "JVM stdout: /var/folders/k_/kpp1czqs3957vq2pr5qngck00000gn/T/tmp2t03b5ln/h2o_laurend_started_from_python.out\n",
      "JVM stderr: /var/folders/k_/kpp1czqs3957vq2pr5qngck00000gn/T/tmpvtztzyoa/h2o_laurend_started_from_python.err\n",
      "Using ice_root: /var/folders/k_/kpp1czqs3957vq2pr5qngck00000gn/T/tmpxlwqpn0y\n",
      "\n",
      "\n",
      "Java Version: java version \"1.8.0_73\"\n",
      "Java(TM) SE Runtime Environment (build 1.8.0_73-b02)\n",
      "Java HotSpot(TM) 64-Bit Server VM (build 25.73-b02, mixed mode)\n",
      "\n",
      "\n",
      "Starting H2O JVM and connecting: .............. Connection successful!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O cluster uptime: </td>\n",
       "<td>1 seconds 518 milliseconds </td></tr>\n",
       "<tr><td>H2O cluster version: </td>\n",
       "<td>3.9.1.3460</td></tr>\n",
       "<tr><td>H2O cluster name: </td>\n",
       "<td>H2O_started_from_python_laurend_itz611</td></tr>\n",
       "<tr><td>H2O cluster total nodes: </td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O cluster total free memory: </td>\n",
       "<td>3.56 GB</td></tr>\n",
       "<tr><td>H2O cluster total cores: </td>\n",
       "<td>8</td></tr>\n",
       "<tr><td>H2O cluster allowed cores: </td>\n",
       "<td>8</td></tr>\n",
       "<tr><td>H2O cluster healthy: </td>\n",
       "<td>True</td></tr>\n",
       "<tr><td>H2O Connection ip: </td>\n",
       "<td>127.0.0.1</td></tr>\n",
       "<tr><td>H2O Connection port: </td>\n",
       "<td>54321</td></tr>\n",
       "<tr><td>H2O Connection proxy: </td>\n",
       "<td>None</td></tr>\n",
       "<tr><td>Python Version: </td>\n",
       "<td>3.5.1</td></tr></table></div>"
      ],
      "text/plain": [
       "------------------------------  --------------------------------------\n",
       "H2O cluster uptime:             1 seconds 518 milliseconds\n",
       "H2O cluster version:            3.9.1.3460\n",
       "H2O cluster name:               H2O_started_from_python_laurend_itz611\n",
       "H2O cluster total nodes:        1\n",
       "H2O cluster total free memory:  3.56 GB\n",
       "H2O cluster total cores:        8\n",
       "H2O cluster allowed cores:      8\n",
       "H2O cluster healthy:            True\n",
       "H2O Connection ip:              127.0.0.1\n",
       "H2O Connection port:            54321\n",
       "H2O Connection proxy:\n",
       "Python Version:                 3.5.1\n",
       "------------------------------  --------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Connect to a cluster \n",
    "h2o.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import and Parse airlines data\n",
      "\n",
      "Parse Progress: [##################################################] 100%\n"
     ]
    }
   ],
   "source": [
    "# 1 - Load data - One row per flight.  \n",
    "# Columns include origin, destination, departure & arrival times, carrier information, and whether flight was delayed.\n",
    "print(\"Import and Parse airlines data\")\n",
    "# air_path = 'allyears2k_headers.zip'\n",
    "air_path = \"http://h2o-public-test-data.s3.amazonaws.com/smalldata/airlines/allyears2k_headers.zip\"\n",
    "data = h2o.import_file(path = air_path)\n",
    "# data.describe() # uncomment to see summary of loaded data file\n",
    "# data.head()     # uncomment to see top of the loaded data file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Data with Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "glm Model Build Progress: [##################################################] 100%\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZgAAAEPCAYAAAB/WNKuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nunTtp6qV3CMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x115fd56d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 2 - Data exploration and munging. \n",
    "# Generate scatter plots of various columns and plot fitted GLM model.\n",
    "\n",
    "# Function to fit a GLM model and plot the fitted (x,y) values\n",
    "def scatter_plot(data, x, y, max_points = 1000, fit = True):\n",
    "    if(fit):\n",
    "        lr = H2OGeneralizedLinearEstimator(family = \"gaussian\")\n",
    "        lr.train(x=x, y=y, training_frame=data)\n",
    "        coeff = lr.coef()\n",
    "    df = data[[x,y]]\n",
    "    runif = df[y].runif()\n",
    "    df_subset = df[runif < float(max_points)/data.nrow]\n",
    "    df_py = h2o.as_list(df_subset)\n",
    "    \n",
    "    if(fit): h2o.remove(lr._id)\n",
    "\n",
    "    # If x variable is string, generate box-and-whisker plot\n",
    "    if(df_py[x].dtype == \"object\"):\n",
    "        if interactive: df_py.boxplot(column = y, by = x)\n",
    "    # Otherwise, generate a scatter plot\n",
    "    else:\n",
    "        if interactive: df_py.plot(x = x, y = y, kind = \"scatter\")\n",
    "    \n",
    "    if(fit):\n",
    "        x_min = min(df_py[x])\n",
    "        x_max = max(df_py[x])\n",
    "        y_min = coeff[\"Intercept\"] + coeff[x]*x_min\n",
    "        y_max = coeff[\"Intercept\"] + coeff[x]*x_max\n",
    "        plt.plot([x_min, x_max], [y_min, y_max], \"k-\")\n",
    "    if interactive: plt.show()\n",
    "        \n",
    "# generate matplotlib plots inside of ipython notebook        \n",
    "%matplotlib inline  \n",
    "scatter_plot(data, \"Distance\", \"AirTime\", fit = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th style=\"text-align: right;\">  Month</th><th style=\"text-align: right;\">  sum_Cancelled</th><th style=\"text-align: right;\">  nrow_Year</th></tr>\n",
       "<tr><td style=\"text-align: right;\">      1</td><td style=\"text-align: right;\">           1067</td><td style=\"text-align: right;\">      41979</td></tr>\n",
       "<tr><td style=\"text-align: right;\">     10</td><td style=\"text-align: right;\">             19</td><td style=\"text-align: right;\">       1999</td></tr>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows:2 Cols:3\n",
      "\n",
      "Chunk compression summary: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>chunk_type</b></td>\n",
       "<td><b>chunk_name</b></td>\n",
       "<td><b>count</b></td>\n",
       "<td><b>count_percentage</b></td>\n",
       "<td><b>size</b></td>\n",
       "<td><b>size_percentage</b></td></tr>\n",
       "<tr><td>C1N</td>\n",
       "<td>1-Byte Integers (w/o NAs)</td>\n",
       "<td>1</td>\n",
       "<td>33.333336</td>\n",
       "<td>     70  B</td>\n",
       "<td>30.434782</td></tr>\n",
       "<tr><td>C2</td>\n",
       "<td>2-Byte Integers</td>\n",
       "<td>1</td>\n",
       "<td>33.333336</td>\n",
       "<td>     72  B</td>\n",
       "<td>31.304348</td></tr>\n",
       "<tr><td>C2S</td>\n",
       "<td>2-Byte Fractions</td>\n",
       "<td>1</td>\n",
       "<td>33.333336</td>\n",
       "<td>     88  B</td>\n",
       "<td>38.260868</td></tr></table></div>"
      ],
      "text/plain": [
       "chunk_type    chunk_name                 count    count_percentage    size    size_percentage\n",
       "------------  -------------------------  -------  ------------------  ------  -----------------\n",
       "C1N           1-Byte Integers (w/o NAs)  1        33.3333             70  B   30.4348\n",
       "C2            2-Byte Integers            1        33.3333             72  B   31.3043\n",
       "C2S           2-Byte Fractions           1        33.3333             88  B   38.2609"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Frame distribution summary: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>size</b></td>\n",
       "<td><b>number_of_rows</b></td>\n",
       "<td><b>number_of_chunks_per_column</b></td>\n",
       "<td><b>number_of_chunks</b></td></tr>\n",
       "<tr><td>127.0.0.1:54321</td>\n",
       "<td>    230  B</td>\n",
       "<td>2.0</td>\n",
       "<td>1.0</td>\n",
       "<td>3.0</td></tr>\n",
       "<tr><td>mean</td>\n",
       "<td>    230  B</td>\n",
       "<td>2.0</td>\n",
       "<td>1.0</td>\n",
       "<td>3.0</td></tr>\n",
       "<tr><td>min</td>\n",
       "<td>    230  B</td>\n",
       "<td>2.0</td>\n",
       "<td>1.0</td>\n",
       "<td>3.0</td></tr>\n",
       "<tr><td>max</td>\n",
       "<td>    230  B</td>\n",
       "<td>2.0</td>\n",
       "<td>1.0</td>\n",
       "<td>3.0</td></tr>\n",
       "<tr><td>stddev</td>\n",
       "<td>      0  B</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>total</td>\n",
       "<td>    230  B</td>\n",
       "<td>2.0</td>\n",
       "<td>1.0</td>\n",
       "<td>3.0</td></tr></table></div>"
      ],
      "text/plain": [
       "                 size    number_of_rows    number_of_chunks_per_column    number_of_chunks\n",
       "---------------  ------  ----------------  -----------------------------  ------------------\n",
       "127.0.0.1:54321  230  B  2                 1                              3\n",
       "mean             230  B  2                 1                              3\n",
       "min              230  B  2                 1                              3\n",
       "max              230  B  2                 1                              3\n",
       "stddev           0  B    0                 0                              0\n",
       "total            230  B  2                 1                              3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>       </th><th>Month            </th><th>sum_Cancelled    </th><th>nrow_Year        </th></tr>\n",
       "<tr><td>type   </td><td>int              </td><td>int              </td><td>int              </td></tr>\n",
       "<tr><td>mins   </td><td>1.0              </td><td>19.0             </td><td>1999.0           </td></tr>\n",
       "<tr><td>mean   </td><td>5.5              </td><td>543.0            </td><td>21989.0          </td></tr>\n",
       "<tr><td>maxs   </td><td>10.0             </td><td>1067.0           </td><td>41979.0          </td></tr>\n",
       "<tr><td>sigma  </td><td>6.363961030678928</td><td>741.0479066835018</td><td>28270.12911183817</td></tr>\n",
       "<tr><td>zeros  </td><td>0                </td><td>0                </td><td>0                </td></tr>\n",
       "<tr><td>missing</td><td>0                </td><td>0                </td><td>0                </td></tr>\n",
       "<tr><td>0      </td><td>1.0              </td><td>1067.0           </td><td>41979.0          </td></tr>\n",
       "<tr><td>1      </td><td>10.0             </td><td>19.0             </td><td>1999.0           </td></tr>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Group flights by month\n",
    "grouped = data.group_by(\"Month\")\n",
    "bpd = grouped.count().sum(\"Cancelled\").frame\n",
    "bpd.show()\n",
    "bpd.describe()\n",
    "bpd.dim\n",
    "\n",
    "# Convert columns to factors\n",
    "data[\"Year\"]= data[\"Year\"].asfactor()\n",
    "data[\"Month\"] = data[\"Month\"].asfactor()\n",
    "data[\"DayOfWeek\"] = data[\"DayOfWeek\"].asfactor()\n",
    "data[\"Cancelled\"] = data[\"Cancelled\"].asfactor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parse Progress: [##################################################] 100%\n",
      "\n",
      "glm Model Build Progress: [##################################################] 100%\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZgAAAEPCAYAAAB/WNKuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKIqipAUVGEVRFCUtqMAoiqIoaeH/A3hM50S4azBAAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1152ef358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculate and plot travel time\n",
    "hour1 = data[\"CRSArrTime\"] / 100\n",
    "mins1 = data[\"CRSArrTime\"] % 100\n",
    "arrTime = hour1*60 + mins1\n",
    "\n",
    "hour2 = data[\"CRSDepTime\"] / 100\n",
    "mins2 = data[\"CRSDepTime\"] % 100\n",
    "depTime = hour2*60 + mins2\n",
    "\n",
    "data[\"TravelTime\"] = (arrTime-depTime > 0).ifelse((arrTime-depTime), h2o.H2OFrame([[None]] * data.nrow))\n",
    "\n",
    "scatter_plot(data, \"Distance\", \"TravelTime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "glm Model Build Progress: [##################################################] 100%\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZgAAAEPCAYAAAB/WNKuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUR7aahTNAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10f4c4ba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Impute missing travel times and re-plot\n",
    "data.impute(column = \"Distance\", by = [\"Origin\", \"Dest\"])\n",
    "scatter_plot(data, \"Distance\", \"TravelTime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"split\"></a>\n",
    "# Split Data Set into Train and Test Sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 3 - Fit a model on train; using test as validation.\n",
    "# Create test/train split\n",
    "s = data[\"Year\"].runif()\n",
    "train = data[s <= 0.75]\n",
    "test  = data[s > 0.75]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(304, 32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace all instances of 'SFO' in the destination column ('Dest') with 'BB8'\n",
    "test[\"Dest\"] = (test[\"Dest\"] == 'SFO').ifelse('BB8', test[\"Dest\"])\n",
    "# print out the number of rows that were effected\n",
    "test[test['Dest']=='BB8'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">*We replace all instances of 'SFO' from the 'Dest' column, to create the situation in which your test set has a categorical level that was not present in the training set (Note: all models will run without breaking, because new categorical levels are interpreted as if they were NA values)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"build\"></a>\n",
    "# Build Models with Supervised Learning Algorithms \n",
    "\n",
    "Train models for GLM, GBM, DRF, Deep Learning & Naive Bayes in one go! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "glm Model Build Progress: [##################################################] 100%\n",
      "\n",
      "gbm Model Build Progress: [##################################################] 100%\n",
      "\n",
      "drf Model Build Progress: [##################################################] 100%\n",
      "\n",
      "deeplearning Model Build Progress: [##################################################] 100%\n",
      "\n",
      "naivebayes Model Build Progress: [##################################################] 100%\n"
     ]
    }
   ],
   "source": [
    "# Set response column\n",
    "myY = \"IsDepDelayed\"\n",
    "# Set feature columns\n",
    "myX = [\"Origin\", \"Dest\", \"Year\", \"UniqueCarrier\", \"DayOfWeek\", \"Month\", \"Distance\", \"FlightNum\"]\n",
    "\n",
    "# Predict delays with GLM\n",
    "data_glm = H2OGeneralizedLinearEstimator(family=\"binomial\", standardize=True)\n",
    "data_glm.train(x = myX, y = myY, training_frame = train, validation_frame = test)\n",
    "\n",
    "# Predict delays with GBM\n",
    "data_gbm2 = H2OGradientBoostingEstimator(balance_classes = False, ntrees = 50, max_depth = 5,\n",
    "                                         distribution = \"bernoulli\", learn_rate = 0.1, min_rows = 2)\n",
    "data_gbm2.train(x = myX, y = myY, training_frame = train, validation_frame = test)\n",
    "\n",
    "# Predict delays with Distributed Random Forest (DRF)\n",
    "data_rf2 = H2ORandomForestEstimator(ntrees = 10,max_depth = 5, balance_classes = False)\n",
    "data_rf2.train(x = myX, y = myY, training_frame = train, validation_frame = test)\n",
    "\n",
    "# Predict delays with Deep Learning\n",
    "data_dl = H2ODeepLearningEstimator(hidden = [10,10], epochs = 5, variable_importances = True,\n",
    "                                   balance_classes = False, loss = \"Automatic\")\n",
    "data_dl.train(x = myX, y = myY, training_frame = train, validation_frame=test)\n",
    "\n",
    "# Predict delays with Naive Bayes\n",
    "# If laplace smoothing is disabled ('laplace=0') the algorithm will predict 0\n",
    "data_nb = H2ONaiveBayesEstimator(laplace=1) \n",
    "data_nb.train(x = myX, y = myY, training_frame = train, validation_frame=test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Or build models individually "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"GLM Model\"></a>\n",
    "`Build GLM` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "glm Model Build Progress: [##################################################] 100%\n",
      "\n",
      "ModelMetricsBinomialGLM: glm\n",
      "** Reported on test data. **\n",
      "\n",
      "MSE: 0.21416530940414985\n",
      "R^2: 0.14200193566672592\n",
      "LogLoss: 0.616556553874648\n",
      "Null degrees of freedom: 10918\n",
      "Residual degrees of freedom: 10678\n",
      "Null deviance: 15122.125326794045\n",
      "Residual deviance: 13464.362023514564\n",
      "AIC: 13946.362023514564\n",
      "AUC: 0.7166694556732763\n",
      "Gini: 0.43333891134655267\n",
      "\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.331854947054549: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>NO</b></td>\n",
       "<td><b>YES</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>NO</td>\n",
       "<td>1496.0</td>\n",
       "<td>3748.0</td>\n",
       "<td>0.7147</td>\n",
       "<td> (3748.0/5244.0)</td></tr>\n",
       "<tr><td>YES</td>\n",
       "<td>505.0</td>\n",
       "<td>5170.0</td>\n",
       "<td>0.089</td>\n",
       "<td> (505.0/5675.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>2001.0</td>\n",
       "<td>8918.0</td>\n",
       "<td>0.3895</td>\n",
       "<td> (4253.0/10919.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       NO    YES    Error    Rate\n",
       "-----  ----  -----  -------  ----------------\n",
       "NO     1496  3748   0.7147   (3748.0/5244.0)\n",
       "YES    505   5170   0.089    (505.0/5675.0)\n",
       "Total  2001  8918   0.3895   (4253.0/10919.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.3318549</td>\n",
       "<td>0.7085589</td>\n",
       "<td>314.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.1648148</td>\n",
       "<td>0.8451715</td>\n",
       "<td>379.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.5324188</td>\n",
       "<td>0.6755473</td>\n",
       "<td>203.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.5097744</td>\n",
       "<td>0.6633391</td>\n",
       "<td>216.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.9797664</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0806012</td>\n",
       "<td>1.0</td>\n",
       "<td>399.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.9797664</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_MCC</td>\n",
       "<td>0.5097744</td>\n",
       "<td>0.3251416</td>\n",
       "<td>216.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.5269278</td>\n",
       "<td>0.6601831</td>\n",
       "<td>207.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                      threshold    value     idx\n",
       "--------------------------  -----------  --------  -----\n",
       "max f1                      0.331855     0.708559  314\n",
       "max f2                      0.164815     0.845172  379\n",
       "max f0point5                0.532419     0.675547  203\n",
       "max accuracy                0.509774     0.663339  216\n",
       "max precision               0.979766     1         0\n",
       "max recall                  0.0806012    1         399\n",
       "max specificity             0.979766     1         0\n",
       "max absolute_MCC            0.509774     0.325142  216\n",
       "max min_per_class_accuracy  0.526928     0.660183  207"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gains/Lift Table: Avg response rate: 51.97 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0100742</td>\n",
       "<td>0.9115665</td>\n",
       "<td>1.8540873</td>\n",
       "<td>1.8540873</td>\n",
       "<td>0.9636364</td>\n",
       "<td>0.9636364</td>\n",
       "<td>0.0186784</td>\n",
       "<td>0.0186784</td>\n",
       "<td>85.4087305</td>\n",
       "<td>85.4087305</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0200568</td>\n",
       "<td>0.8818044</td>\n",
       "<td>1.6239712</td>\n",
       "<td>1.7395546</td>\n",
       "<td>0.8440367</td>\n",
       "<td>0.9041096</td>\n",
       "<td>0.0162115</td>\n",
       "<td>0.0348899</td>\n",
       "<td>62.3971224</td>\n",
       "<td>73.9554644</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0300394</td>\n",
       "<td>0.8647063</td>\n",
       "<td>1.7828380</td>\n",
       "<td>1.7539384</td>\n",
       "<td>0.9266055</td>\n",
       "<td>0.9115854</td>\n",
       "<td>0.0177974</td>\n",
       "<td>0.0526872</td>\n",
       "<td>78.2837974</td>\n",
       "<td>75.3938433</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0400220</td>\n",
       "<td>0.8536446</td>\n",
       "<td>1.6769268</td>\n",
       "<td>1.7347296</td>\n",
       "<td>0.8715596</td>\n",
       "<td>0.9016018</td>\n",
       "<td>0.0167401</td>\n",
       "<td>0.0694273</td>\n",
       "<td>67.6926808</td>\n",
       "<td>73.4729584</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0500046</td>\n",
       "<td>0.8431967</td>\n",
       "<td>1.5180601</td>\n",
       "<td>1.6914750</td>\n",
       "<td>0.7889908</td>\n",
       "<td>0.8791209</td>\n",
       "<td>0.0151542</td>\n",
       "<td>0.0845815</td>\n",
       "<td>51.8060057</td>\n",
       "<td>69.1475045</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1000092</td>\n",
       "<td>0.7799468</td>\n",
       "<td>1.5364232</td>\n",
       "<td>1.6139491</td>\n",
       "<td>0.7985348</td>\n",
       "<td>0.8388278</td>\n",
       "<td>0.0768282</td>\n",
       "<td>0.1614097</td>\n",
       "<td>53.6423166</td>\n",
       "<td>61.3949105</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1500137</td>\n",
       "<td>0.7336259</td>\n",
       "<td>1.4694689</td>\n",
       "<td>1.5657891</td>\n",
       "<td>0.7637363</td>\n",
       "<td>0.8137973</td>\n",
       "<td>0.0734802</td>\n",
       "<td>0.2348899</td>\n",
       "<td>46.9468945</td>\n",
       "<td>56.5789052</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2001099</td>\n",
       "<td>0.6998976</td>\n",
       "<td>1.3120141</td>\n",
       "<td>1.5022582</td>\n",
       "<td>0.6819013</td>\n",
       "<td>0.7807780</td>\n",
       "<td>0.0657269</td>\n",
       "<td>0.3006167</td>\n",
       "<td>31.2014110</td>\n",
       "<td>50.2258208</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3000275</td>\n",
       "<td>0.6405624</td>\n",
       "<td>1.2662419</td>\n",
       "<td>1.4236582</td>\n",
       "<td>0.6581118</td>\n",
       "<td>0.7399267</td>\n",
       "<td>0.1265198</td>\n",
       "<td>0.4271366</td>\n",
       "<td>26.6241939</td>\n",
       "<td>42.3658163</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4000366</td>\n",
       "<td>0.5861789</td>\n",
       "<td>1.2245575</td>\n",
       "<td>1.3738830</td>\n",
       "<td>0.6364469</td>\n",
       "<td>0.7140568</td>\n",
       "<td>0.1224670</td>\n",
       "<td>0.5496035</td>\n",
       "<td>22.4557454</td>\n",
       "<td>37.3882986</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5000458</td>\n",
       "<td>0.5303817</td>\n",
       "<td>1.0501241</td>\n",
       "<td>1.3091312</td>\n",
       "<td>0.5457875</td>\n",
       "<td>0.6804029</td>\n",
       "<td>0.1050220</td>\n",
       "<td>0.6546256</td>\n",
       "<td>5.0124090</td>\n",
       "<td>30.9131207</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.5999634</td>\n",
       "<td>0.4603878</td>\n",
       "<td>0.9470361</td>\n",
       "<td>1.2488281</td>\n",
       "<td>0.4922090</td>\n",
       "<td>0.6490612</td>\n",
       "<td>0.0946256</td>\n",
       "<td>0.7492511</td>\n",
       "<td>-5.2963898</td>\n",
       "<td>24.8828084</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.6999725</td>\n",
       "<td>0.4027967</td>\n",
       "<td>0.8087365</td>\n",
       "<td>1.1859496</td>\n",
       "<td>0.4203297</td>\n",
       "<td>0.6163810</td>\n",
       "<td>0.0808811</td>\n",
       "<td>0.8301322</td>\n",
       "<td>-19.1263494</td>\n",
       "<td>18.5949632</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.7999817</td>\n",
       "<td>0.3422599</td>\n",
       "<td>0.6889237</td>\n",
       "<td>1.1238143</td>\n",
       "<td>0.3580586</td>\n",
       "<td>0.5840870</td>\n",
       "<td>0.0688987</td>\n",
       "<td>0.8990308</td>\n",
       "<td>-31.1076310</td>\n",
       "<td>12.3814277</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.8999908</td>\n",
       "<td>0.2686606</td>\n",
       "<td>0.5796826</td>\n",
       "<td>1.0633490</td>\n",
       "<td>0.3012821</td>\n",
       "<td>0.5526610</td>\n",
       "<td>0.0579736</td>\n",
       "<td>0.9570044</td>\n",
       "<td>-42.0317407</td>\n",
       "<td>6.3349049</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0797865</td>\n",
       "<td>0.4299166</td>\n",
       "<td>1.0</td>\n",
       "<td>0.2234432</td>\n",
       "<td>0.5197362</td>\n",
       "<td>0.0429956</td>\n",
       "<td>1.0</td>\n",
       "<td>-57.0083426</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift      cumulative_lift    response_rate    cumulative_response_rate    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  --------  -----------------  ---------------  --------------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.0100742                   0.911567           1.85409   1.85409            0.963636         0.963636                    0.0186784       0.0186784                  85.4087   85.4087\n",
       "    2        0.0200568                   0.881804           1.62397   1.73955            0.844037         0.90411                     0.0162115       0.0348899                  62.3971   73.9555\n",
       "    3        0.0300394                   0.864706           1.78284   1.75394            0.926606         0.911585                    0.0177974       0.0526872                  78.2838   75.3938\n",
       "    4        0.040022                    0.853645           1.67693   1.73473            0.87156          0.901602                    0.0167401       0.0694273                  67.6927   73.473\n",
       "    5        0.0500046                   0.843197           1.51806   1.69148            0.788991         0.879121                    0.0151542       0.0845815                  51.806    69.1475\n",
       "    6        0.100009                    0.779947           1.53642   1.61395            0.798535         0.838828                    0.0768282       0.16141                    53.6423   61.3949\n",
       "    7        0.150014                    0.733626           1.46947   1.56579            0.763736         0.813797                    0.0734802       0.23489                    46.9469   56.5789\n",
       "    8        0.20011                     0.699898           1.31201   1.50226            0.681901         0.780778                    0.0657269       0.300617                   31.2014   50.2258\n",
       "    9        0.300027                    0.640562           1.26624   1.42366            0.658112         0.739927                    0.12652         0.427137                   26.6242   42.3658\n",
       "    10       0.400037                    0.586179           1.22456   1.37388            0.636447         0.714057                    0.122467        0.549604                   22.4557   37.3883\n",
       "    11       0.500046                    0.530382           1.05012   1.30913            0.545788         0.680403                    0.105022        0.654626                   5.01241   30.9131\n",
       "    12       0.599963                    0.460388           0.947036  1.24883            0.492209         0.649061                    0.0946256       0.749251                   -5.29639  24.8828\n",
       "    13       0.699973                    0.402797           0.808737  1.18595            0.42033          0.616381                    0.0808811       0.830132                   -19.1263  18.595\n",
       "    14       0.799982                    0.34226            0.688924  1.12381            0.358059         0.584087                    0.0688987       0.899031                   -31.1076  12.3814\n",
       "    15       0.899991                    0.268661           0.579683  1.06335            0.301282         0.552661                    0.0579736       0.957004                   -42.0317  6.3349\n",
       "    16       1                           0.0797865          0.429917  1                  0.223443         0.519736                    0.0429956       1                          -57.0083  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set response column\n",
    "myY = \"IsDepDelayed\"\n",
    "# Set feature columns\n",
    "myX = [\"Origin\", \"Dest\", \"Year\", \"UniqueCarrier\", \"DayOfWeek\", \"Month\", \"Distance\", \"FlightNum\"]\n",
    "\n",
    "# Predict delays with GLM\n",
    "data_glm = H2OGeneralizedLinearEstimator(family=\"binomial\", standardize=True)\n",
    "data_glm.train(x = myX, y = myY, training_frame = train, validation_frame = test)\n",
    "\n",
    "data_glm.model_performance(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "glm prediction Progress: [##################################################] 100%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>predict  </th><th style=\"text-align: right;\">      NO</th><th style=\"text-align: right;\">     YES</th></tr>\n",
       "<tr><td>YES      </td><td style=\"text-align: right;\">0.462817</td><td style=\"text-align: right;\">0.537183</td></tr>\n",
       "<tr><td>YES      </td><td style=\"text-align: right;\">0.426812</td><td style=\"text-align: right;\">0.573188</td></tr>\n",
       "<tr><td>YES      </td><td style=\"text-align: right;\">0.293095</td><td style=\"text-align: right;\">0.706905</td></tr>\n",
       "<tr><td>YES      </td><td style=\"text-align: right;\">0.351103</td><td style=\"text-align: right;\">0.648897</td></tr>\n",
       "<tr><td>YES      </td><td style=\"text-align: right;\">0.386998</td><td style=\"text-align: right;\">0.613002</td></tr>\n",
       "<tr><td>YES      </td><td style=\"text-align: right;\">0.356089</td><td style=\"text-align: right;\">0.643911</td></tr>\n",
       "<tr><td>YES      </td><td style=\"text-align: right;\">0.318634</td><td style=\"text-align: right;\">0.681366</td></tr>\n",
       "<tr><td>YES      </td><td style=\"text-align: right;\">0.471821</td><td style=\"text-align: right;\">0.528179</td></tr>\n",
       "<tr><td>YES      </td><td style=\"text-align: right;\">0.49142 </td><td style=\"text-align: right;\">0.50858 </td></tr>\n",
       "<tr><td>YES      </td><td style=\"text-align: right;\">0.529944</td><td style=\"text-align: right;\">0.470056</td></tr>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glm_pred_output = data_glm.predict(test)\n",
    "glm_pred_output.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"GBM Model\"></a>\n",
    "`Build GBM`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "gbm Model Build Progress: [##################################################] 100%\n",
      "\n",
      "ModelMetricsBinomial: gbm\n",
      "** Reported on test data. **\n",
      "\n",
      "MSE: 0.20994431051192827\n",
      "R^2: 0.1589122788458105\n",
      "LogLoss: 0.6074948423093912\n",
      "AUC: 0.7296870936198954\n",
      "Gini: 0.45937418723979073\n",
      "\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.37325809173716235: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>NO</b></td>\n",
       "<td><b>YES</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>NO</td>\n",
       "<td>2249.0</td>\n",
       "<td>2995.0</td>\n",
       "<td>0.5711</td>\n",
       "<td> (2995.0/5244.0)</td></tr>\n",
       "<tr><td>YES</td>\n",
       "<td>839.0</td>\n",
       "<td>4836.0</td>\n",
       "<td>0.1478</td>\n",
       "<td> (839.0/5675.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>3088.0</td>\n",
       "<td>7831.0</td>\n",
       "<td>0.3511</td>\n",
       "<td> (3834.0/10919.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       NO    YES    Error    Rate\n",
       "-----  ----  -----  -------  ----------------\n",
       "NO     2249  2995   0.5711   (2995.0/5244.0)\n",
       "YES    839   4836   0.1478   (839.0/5675.0)\n",
       "Total  3088  7831   0.3511   (3834.0/10919.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.3732581</td>\n",
       "<td>0.7161262</td>\n",
       "<td>285.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.1949921</td>\n",
       "<td>0.8460707</td>\n",
       "<td>369.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.5645440</td>\n",
       "<td>0.6866944</td>\n",
       "<td>177.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.4746806</td>\n",
       "<td>0.6727722</td>\n",
       "<td>226.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.9572355</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0846539</td>\n",
       "<td>1.0</td>\n",
       "<td>398.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.9572355</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_MCC</td>\n",
       "<td>0.4746806</td>\n",
       "<td>0.3435593</td>\n",
       "<td>226.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.5078917</td>\n",
       "<td>0.6697178</td>\n",
       "<td>209.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                      threshold    value     idx\n",
       "--------------------------  -----------  --------  -----\n",
       "max f1                      0.373258     0.716126  285\n",
       "max f2                      0.194992     0.846071  369\n",
       "max f0point5                0.564544     0.686694  177\n",
       "max accuracy                0.474681     0.672772  226\n",
       "max precision               0.957236     1         0\n",
       "max recall                  0.0846539    1         398\n",
       "max specificity             0.957236     1         0\n",
       "max absolute_MCC            0.474681     0.343559  226\n",
       "max min_per_class_accuracy  0.507892     0.669718  209"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gains/Lift Table: Avg response rate: 51.97 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0100742</td>\n",
       "<td>0.8994217</td>\n",
       "<td>1.8191045</td>\n",
       "<td>1.8191045</td>\n",
       "<td>0.9454545</td>\n",
       "<td>0.9454545</td>\n",
       "<td>0.0183260</td>\n",
       "<td>0.0183260</td>\n",
       "<td>81.9104525</td>\n",
       "<td>81.9104525</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0200568</td>\n",
       "<td>0.8855835</td>\n",
       "<td>1.6769268</td>\n",
       "<td>1.7483403</td>\n",
       "<td>0.8715596</td>\n",
       "<td>0.9086758</td>\n",
       "<td>0.0167401</td>\n",
       "<td>0.0350661</td>\n",
       "<td>67.6926808</td>\n",
       "<td>74.8340273</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0300394</td>\n",
       "<td>0.8727916</td>\n",
       "<td>1.7475343</td>\n",
       "<td>1.7480724</td>\n",
       "<td>0.9082569</td>\n",
       "<td>0.9085366</td>\n",
       "<td>0.0174449</td>\n",
       "<td>0.0525110</td>\n",
       "<td>74.7534252</td>\n",
       "<td>74.8072419</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0401136</td>\n",
       "<td>0.8609245</td>\n",
       "<td>1.7316476</td>\n",
       "<td>1.7439475</td>\n",
       "<td>0.9</td>\n",
       "<td>0.9063927</td>\n",
       "<td>0.0174449</td>\n",
       "<td>0.0699559</td>\n",
       "<td>73.1647577</td>\n",
       "<td>74.3947458</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0500046</td>\n",
       "<td>0.8495728</td>\n",
       "<td>1.6568233</td>\n",
       "<td>1.7267141</td>\n",
       "<td>0.8611111</td>\n",
       "<td>0.8974359</td>\n",
       "<td>0.0163877</td>\n",
       "<td>0.0863436</td>\n",
       "<td>65.6823299</td>\n",
       "<td>72.6714108</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1000092</td>\n",
       "<td>0.7923066</td>\n",
       "<td>1.5082319</td>\n",
       "<td>1.6174730</td>\n",
       "<td>0.7838828</td>\n",
       "<td>0.8406593</td>\n",
       "<td>0.0754185</td>\n",
       "<td>0.1617621</td>\n",
       "<td>50.8231915</td>\n",
       "<td>61.7473012</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1504717</td>\n",
       "<td>0.7499476</td>\n",
       "<td>1.4456586</td>\n",
       "<td>1.5598530</td>\n",
       "<td>0.7513612</td>\n",
       "<td>0.8107121</td>\n",
       "<td>0.0729515</td>\n",
       "<td>0.2347137</td>\n",
       "<td>44.5658594</td>\n",
       "<td>55.9852960</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2001099</td>\n",
       "<td>0.7143123</td>\n",
       "<td>1.4057656</td>\n",
       "<td>1.5216308</td>\n",
       "<td>0.7306273</td>\n",
       "<td>0.7908467</td>\n",
       "<td>0.0697797</td>\n",
       "<td>0.3044934</td>\n",
       "<td>40.5765561</td>\n",
       "<td>52.1630823</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3001191</td>\n",
       "<td>0.6513619</td>\n",
       "<td>1.3478942</td>\n",
       "<td>1.4637363</td>\n",
       "<td>0.7005495</td>\n",
       "<td>0.7607568</td>\n",
       "<td>0.1348018</td>\n",
       "<td>0.4392952</td>\n",
       "<td>34.7894176</td>\n",
       "<td>46.3736280</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4001282</td>\n",
       "<td>0.5829166</td>\n",
       "<td>1.2016521</td>\n",
       "<td>1.3982302</td>\n",
       "<td>0.6245421</td>\n",
       "<td>0.7267109</td>\n",
       "<td>0.1201762</td>\n",
       "<td>0.5594714</td>\n",
       "<td>20.1652063</td>\n",
       "<td>39.8230222</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5001374</td>\n",
       "<td>0.5111190</td>\n",
       "<td>1.0342665</td>\n",
       "<td>1.3254508</td>\n",
       "<td>0.5375458</td>\n",
       "<td>0.6888848</td>\n",
       "<td>0.1034361</td>\n",
       "<td>0.6629075</td>\n",
       "<td>3.4266512</td>\n",
       "<td>32.5450810</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.5999634</td>\n",
       "<td>0.4450936</td>\n",
       "<td>0.9602612</td>\n",
       "<td>1.2646881</td>\n",
       "<td>0.4990826</td>\n",
       "<td>0.6573042</td>\n",
       "<td>0.0958590</td>\n",
       "<td>0.7587665</td>\n",
       "<td>-3.9738754</td>\n",
       "<td>26.4688083</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.6999725</td>\n",
       "<td>0.3819586</td>\n",
       "<td>0.8016887</td>\n",
       "<td>1.1985367</td>\n",
       "<td>0.4166667</td>\n",
       "<td>0.6229229</td>\n",
       "<td>0.0801762</td>\n",
       "<td>0.8389427</td>\n",
       "<td>-19.8311307</td>\n",
       "<td>19.8536659</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.7999817</td>\n",
       "<td>0.3246684</td>\n",
       "<td>0.6448749</td>\n",
       "<td>1.1293210</td>\n",
       "<td>0.3351648</td>\n",
       "<td>0.5869491</td>\n",
       "<td>0.0644934</td>\n",
       "<td>0.9034361</td>\n",
       "<td>-35.5125139</td>\n",
       "<td>12.9321011</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.8999908</td>\n",
       "<td>0.2601986</td>\n",
       "<td>0.5497294</td>\n",
       "<td>1.0649154</td>\n",
       "<td>0.2857143</td>\n",
       "<td>0.5534751</td>\n",
       "<td>0.0549780</td>\n",
       "<td>0.9584141</td>\n",
       "<td>-45.0270610</td>\n",
       "<td>6.4915389</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0799673</td>\n",
       "<td>0.4158209</td>\n",
       "<td>1.0</td>\n",
       "<td>0.2161172</td>\n",
       "<td>0.5197362</td>\n",
       "<td>0.0415859</td>\n",
       "<td>1.0</td>\n",
       "<td>-58.4179051</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift      cumulative_lift    response_rate    cumulative_response_rate    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  --------  -----------------  ---------------  --------------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.0100742                   0.899422           1.8191    1.8191             0.945455         0.945455                    0.018326        0.018326                   81.9105   81.9105\n",
       "    2        0.0200568                   0.885584           1.67693   1.74834            0.87156          0.908676                    0.0167401       0.0350661                  67.6927   74.834\n",
       "    3        0.0300394                   0.872792           1.74753   1.74807            0.908257         0.908537                    0.0174449       0.052511                   74.7534   74.8072\n",
       "    4        0.0401136                   0.860924           1.73165   1.74395            0.9              0.906393                    0.0174449       0.0699559                  73.1648   74.3947\n",
       "    5        0.0500046                   0.849573           1.65682   1.72671            0.861111         0.897436                    0.0163877       0.0863436                  65.6823   72.6714\n",
       "    6        0.100009                    0.792307           1.50823   1.61747            0.783883         0.840659                    0.0754185       0.161762                   50.8232   61.7473\n",
       "    7        0.150472                    0.749948           1.44566   1.55985            0.751361         0.810712                    0.0729515       0.234714                   44.5659   55.9853\n",
       "    8        0.20011                     0.714312           1.40577   1.52163            0.730627         0.790847                    0.0697797       0.304493                   40.5766   52.1631\n",
       "    9        0.300119                    0.651362           1.34789   1.46374            0.700549         0.760757                    0.134802        0.439295                   34.7894   46.3736\n",
       "    10       0.400128                    0.582917           1.20165   1.39823            0.624542         0.726711                    0.120176        0.559471                   20.1652   39.823\n",
       "    11       0.500137                    0.511119           1.03427   1.32545            0.537546         0.688885                    0.103436        0.662907                   3.42665   32.5451\n",
       "    12       0.599963                    0.445094           0.960261  1.26469            0.499083         0.657304                    0.095859        0.758767                   -3.97388  26.4688\n",
       "    13       0.699973                    0.381959           0.801689  1.19854            0.416667         0.622923                    0.0801762       0.838943                   -19.8311  19.8537\n",
       "    14       0.799982                    0.324668           0.644875  1.12932            0.335165         0.586949                    0.0644934       0.903436                   -35.5125  12.9321\n",
       "    15       0.899991                    0.260199           0.549729  1.06492            0.285714         0.553475                    0.054978        0.958414                   -45.0271  6.49154\n",
       "    16       1                           0.0799673          0.415821  1                  0.216117         0.519736                    0.0415859       1                          -58.4179  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set response column\n",
    "myY = \"IsDepDelayed\"\n",
    "# Set feature columns\n",
    "myX = [\"Origin\", \"Dest\", \"Year\", \"UniqueCarrier\", \"DayOfWeek\", \"Month\", \"Distance\", \"FlightNum\"]\n",
    "\n",
    "# Predict delays with GBM\n",
    "data_gbm2 = H2OGradientBoostingEstimator(balance_classes = False, ntrees = 50, max_depth = 5,\n",
    "                                         distribution = \"bernoulli\", learn_rate = 0.1, min_rows = 2)\n",
    "data_gbm2.train(x = myX, y = myY, training_frame = train, validation_frame = test)\n",
    "\n",
    "data_gbm2.model_performance(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "gbm prediction Progress: [##################################################] 100%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>predict  </th><th style=\"text-align: right;\">      NO</th><th style=\"text-align: right;\">     YES</th></tr>\n",
       "<tr><td>NO       </td><td style=\"text-align: right;\">0.709207</td><td style=\"text-align: right;\">0.290793</td></tr>\n",
       "<tr><td>NO       </td><td style=\"text-align: right;\">0.696442</td><td style=\"text-align: right;\">0.303558</td></tr>\n",
       "<tr><td>YES      </td><td style=\"text-align: right;\">0.275673</td><td style=\"text-align: right;\">0.724327</td></tr>\n",
       "<tr><td>YES      </td><td style=\"text-align: right;\">0.31818 </td><td style=\"text-align: right;\">0.68182 </td></tr>\n",
       "<tr><td>YES      </td><td style=\"text-align: right;\">0.316426</td><td style=\"text-align: right;\">0.683574</td></tr>\n",
       "<tr><td>YES      </td><td style=\"text-align: right;\">0.310093</td><td style=\"text-align: right;\">0.689907</td></tr>\n",
       "<tr><td>YES      </td><td style=\"text-align: right;\">0.281212</td><td style=\"text-align: right;\">0.718788</td></tr>\n",
       "<tr><td>YES      </td><td style=\"text-align: right;\">0.415032</td><td style=\"text-align: right;\">0.584968</td></tr>\n",
       "<tr><td>YES      </td><td style=\"text-align: right;\">0.455974</td><td style=\"text-align: right;\">0.544026</td></tr>\n",
       "<tr><td>YES      </td><td style=\"text-align: right;\">0.491473</td><td style=\"text-align: right;\">0.508527</td></tr>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_gbm2.predict(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"DRF Model\"></a>\n",
    "`Build Distributed Random Forest`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "drf Model Build Progress: [##################################################] 100%\n",
      "\n",
      "ModelMetricsBinomial: drf\n",
      "** Reported on test data. **\n",
      "\n",
      "MSE: 0.21599360206518958\n",
      "R^2: 0.1346773527612536\n",
      "LogLoss: 0.6214047527338338\n",
      "AUC: 0.7177846214847596\n",
      "Gini: 0.43556924296951927\n",
      "\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.4300728998567959: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>NO</b></td>\n",
       "<td><b>YES</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>NO</td>\n",
       "<td>2368.0</td>\n",
       "<td>2876.0</td>\n",
       "<td>0.5484</td>\n",
       "<td> (2876.0/5244.0)</td></tr>\n",
       "<tr><td>YES</td>\n",
       "<td>985.0</td>\n",
       "<td>4690.0</td>\n",
       "<td>0.1736</td>\n",
       "<td> (985.0/5675.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>3353.0</td>\n",
       "<td>7566.0</td>\n",
       "<td>0.3536</td>\n",
       "<td> (3861.0/10919.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       NO    YES    Error    Rate\n",
       "-----  ----  -----  -------  ----------------\n",
       "NO     2368  2876   0.5484   (2876.0/5244.0)\n",
       "YES    985   4690   0.1736   (985.0/5675.0)\n",
       "Total  3353  7566   0.3536   (3861.0/10919.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.4300729</td>\n",
       "<td>0.7084057</td>\n",
       "<td>301.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.3002093</td>\n",
       "<td>0.8444856</td>\n",
       "<td>386.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.5293900</td>\n",
       "<td>0.6774533</td>\n",
       "<td>232.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.4886548</td>\n",
       "<td>0.6656287</td>\n",
       "<td>264.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.8944711</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.2455635</td>\n",
       "<td>1.0</td>\n",
       "<td>398.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.8944711</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_MCC</td>\n",
       "<td>0.4886548</td>\n",
       "<td>0.3292966</td>\n",
       "<td>264.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.5210644</td>\n",
       "<td>0.6616740</td>\n",
       "<td>238.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                      threshold    value     idx\n",
       "--------------------------  -----------  --------  -----\n",
       "max f1                      0.430073     0.708406  301\n",
       "max f2                      0.300209     0.844486  386\n",
       "max f0point5                0.52939      0.677453  232\n",
       "max accuracy                0.488655     0.665629  264\n",
       "max precision               0.894471     1         0\n",
       "max recall                  0.245563     1         398\n",
       "max specificity             0.894471     1         0\n",
       "max absolute_MCC            0.488655     0.329297  264\n",
       "max min_per_class_accuracy  0.521064     0.661674  238"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gains/Lift Table: Avg response rate: 51.97 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0100742</td>\n",
       "<td>0.8454385</td>\n",
       "<td>1.8016131</td>\n",
       "<td>1.8016131</td>\n",
       "<td>0.9363636</td>\n",
       "<td>0.9363636</td>\n",
       "<td>0.0181498</td>\n",
       "<td>0.0181498</td>\n",
       "<td>80.1613136</td>\n",
       "<td>80.1613136</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0202399</td>\n",
       "<td>0.8320967</td>\n",
       "<td>1.8373838</td>\n",
       "<td>1.8195794</td>\n",
       "<td>0.9549550</td>\n",
       "<td>0.9457014</td>\n",
       "<td>0.0186784</td>\n",
       "<td>0.0368282</td>\n",
       "<td>83.7383816</td>\n",
       "<td>81.9579405</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0303141</td>\n",
       "<td>0.8236593</td>\n",
       "<td>1.6092078</td>\n",
       "<td>1.7496674</td>\n",
       "<td>0.8363636</td>\n",
       "<td>0.9093656</td>\n",
       "<td>0.0162115</td>\n",
       "<td>0.0530396</td>\n",
       "<td>60.9207849</td>\n",
       "<td>74.9667408</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0401136</td>\n",
       "<td>0.8111829</td>\n",
       "<td>1.7082712</td>\n",
       "<td>1.7395546</td>\n",
       "<td>0.8878505</td>\n",
       "<td>0.9041096</td>\n",
       "<td>0.0167401</td>\n",
       "<td>0.0697797</td>\n",
       "<td>70.8271234</td>\n",
       "<td>73.9554644</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0500046</td>\n",
       "<td>0.8015050</td>\n",
       "<td>1.6033774</td>\n",
       "<td>1.7126185</td>\n",
       "<td>0.8333333</td>\n",
       "<td>0.8901099</td>\n",
       "<td>0.0158590</td>\n",
       "<td>0.0856388</td>\n",
       "<td>60.3377386</td>\n",
       "<td>71.2618483</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1001007</td>\n",
       "<td>0.7491098</td>\n",
       "<td>1.5300969</td>\n",
       "<td>1.6212742</td>\n",
       "<td>0.7952468</td>\n",
       "<td>0.8426349</td>\n",
       "<td>0.0766520</td>\n",
       "<td>0.1622907</td>\n",
       "<td>53.0096884</td>\n",
       "<td>62.1274188</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1501053</td>\n",
       "<td>0.7138253</td>\n",
       "<td>1.4659450</td>\n",
       "<td>1.5695294</td>\n",
       "<td>0.7619048</td>\n",
       "<td>0.8157413</td>\n",
       "<td>0.0733040</td>\n",
       "<td>0.2355947</td>\n",
       "<td>46.5945039</td>\n",
       "<td>56.9529395</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2001099</td>\n",
       "<td>0.6809330</td>\n",
       "<td>1.3884191</td>\n",
       "<td>1.5242725</td>\n",
       "<td>0.7216117</td>\n",
       "<td>0.7922197</td>\n",
       "<td>0.0694273</td>\n",
       "<td>0.3050220</td>\n",
       "<td>38.8419099</td>\n",
       "<td>52.4272543</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3000275</td>\n",
       "<td>0.6356334</td>\n",
       "<td>1.2609512</td>\n",
       "<td>1.4365792</td>\n",
       "<td>0.6553621</td>\n",
       "<td>0.7466422</td>\n",
       "<td>0.1259912</td>\n",
       "<td>0.4310132</td>\n",
       "<td>26.0951235</td>\n",
       "<td>43.6579153</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4000366</td>\n",
       "<td>0.5751088</td>\n",
       "<td>1.1681750</td>\n",
       "<td>1.3694781</td>\n",
       "<td>0.6071429</td>\n",
       "<td>0.7117674</td>\n",
       "<td>0.1168282</td>\n",
       "<td>0.5478414</td>\n",
       "<td>16.8174953</td>\n",
       "<td>36.9478103</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5042586</td>\n",
       "<td>0.5210011</td>\n",
       "<td>1.0837591</td>\n",
       "<td>1.3104247</td>\n",
       "<td>0.5632689</td>\n",
       "<td>0.6810752</td>\n",
       "<td>0.1129515</td>\n",
       "<td>0.6607930</td>\n",
       "<td>8.3759126</td>\n",
       "<td>31.0424671</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.6003297</td>\n",
       "<td>0.4725114</td>\n",
       "<td>0.9335967</td>\n",
       "<td>1.2501207</td>\n",
       "<td>0.4852240</td>\n",
       "<td>0.6497330</td>\n",
       "<td>0.0896916</td>\n",
       "<td>0.7504846</td>\n",
       "<td>-6.6403329</td>\n",
       "<td>25.0120693</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.7010715</td>\n",
       "<td>0.4275092</td>\n",
       "<td>0.7941091</td>\n",
       "<td>1.1845932</td>\n",
       "<td>0.4127273</td>\n",
       "<td>0.6156760</td>\n",
       "<td>0.08</td>\n",
       "<td>0.8304846</td>\n",
       "<td>-20.5890909</td>\n",
       "<td>18.4593226</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.8003480</td>\n",
       "<td>0.3929065</td>\n",
       "<td>0.6691586</td>\n",
       "<td>1.1206579</td>\n",
       "<td>0.3477860</td>\n",
       "<td>0.5824465</td>\n",
       "<td>0.0664317</td>\n",
       "<td>0.8969163</td>\n",
       "<td>-33.0841393</td>\n",
       "<td>12.0657864</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.9001740</td>\n",
       "<td>0.3614699</td>\n",
       "<td>0.5842766</td>\n",
       "<td>1.0611752</td>\n",
       "<td>0.3036697</td>\n",
       "<td>0.5515312</td>\n",
       "<td>0.0583260</td>\n",
       "<td>0.9552423</td>\n",
       "<td>-41.5723397</td>\n",
       "<td>6.1175152</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.2313936</td>\n",
       "<td>0.4483573</td>\n",
       "<td>1.0</td>\n",
       "<td>0.2330275</td>\n",
       "<td>0.5197362</td>\n",
       "<td>0.0447577</td>\n",
       "<td>1.0</td>\n",
       "<td>-55.1642727</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift      cumulative_lift    response_rate    cumulative_response_rate    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  --------  -----------------  ---------------  --------------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.0100742                   0.845438           1.80161   1.80161            0.936364         0.936364                    0.0181498       0.0181498                  80.1613   80.1613\n",
       "    2        0.0202399                   0.832097           1.83738   1.81958            0.954955         0.945701                    0.0186784       0.0368282                  83.7384   81.9579\n",
       "    3        0.0303141                   0.823659           1.60921   1.74967            0.836364         0.909366                    0.0162115       0.0530396                  60.9208   74.9667\n",
       "    4        0.0401136                   0.811183           1.70827   1.73955            0.88785          0.90411                     0.0167401       0.0697797                  70.8271   73.9555\n",
       "    5        0.0500046                   0.801505           1.60338   1.71262            0.833333         0.89011                     0.015859        0.0856388                  60.3377   71.2618\n",
       "    6        0.100101                    0.74911            1.5301    1.62127            0.795247         0.842635                    0.076652        0.162291                   53.0097   62.1274\n",
       "    7        0.150105                    0.713825           1.46595   1.56953            0.761905         0.815741                    0.073304        0.235595                   46.5945   56.9529\n",
       "    8        0.20011                     0.680933           1.38842   1.52427            0.721612         0.79222                     0.0694273       0.305022                   38.8419   52.4273\n",
       "    9        0.300027                    0.635633           1.26095   1.43658            0.655362         0.746642                    0.125991        0.431013                   26.0951   43.6579\n",
       "    10       0.400037                    0.575109           1.16817   1.36948            0.607143         0.711767                    0.116828        0.547841                   16.8175   36.9478\n",
       "    11       0.504259                    0.521001           1.08376   1.31042            0.563269         0.681075                    0.112952        0.660793                   8.37591   31.0425\n",
       "    12       0.60033                     0.472511           0.933597  1.25012            0.485224         0.649733                    0.0896916       0.750485                   -6.64033  25.0121\n",
       "    13       0.701072                    0.427509           0.794109  1.18459            0.412727         0.615676                    0.08            0.830485                   -20.5891  18.4593\n",
       "    14       0.800348                    0.392906           0.669159  1.12066            0.347786         0.582447                    0.0664317       0.896916                   -33.0841  12.0658\n",
       "    15       0.900174                    0.36147            0.584277  1.06118            0.30367          0.551531                    0.058326        0.955242                   -41.5723  6.11752\n",
       "    16       1                           0.231394           0.448357  1                  0.233028         0.519736                    0.0447577       1                          -55.1643  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set response column\n",
    "myY = \"IsDepDelayed\"\n",
    "# Set feature columns\n",
    "myX = [\"Origin\", \"Dest\", \"Year\", \"UniqueCarrier\", \"DayOfWeek\", \"Month\", \"Distance\", \"FlightNum\"]\n",
    "\n",
    "# Predict delays with Distributed Random Forest (DRF)\n",
    "data_rf2 = H2ORandomForestEstimator(ntrees = 10,max_depth = 5, balance_classes = False)\n",
    "data_rf2.train(x = myX, y = myY, training_frame = train, validation_frame = test)\n",
    "\n",
    "data_rf2.model_performance(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "drf prediction Progress: [##################################################] 100%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>predict  </th><th style=\"text-align: right;\">      NO</th><th style=\"text-align: right;\">     YES</th></tr>\n",
       "<tr><td>YES      </td><td style=\"text-align: right;\">0.353397</td><td style=\"text-align: right;\">0.646603</td></tr>\n",
       "<tr><td>YES      </td><td style=\"text-align: right;\">0.370599</td><td style=\"text-align: right;\">0.629401</td></tr>\n",
       "<tr><td>YES      </td><td style=\"text-align: right;\">0.263289</td><td style=\"text-align: right;\">0.736711</td></tr>\n",
       "<tr><td>YES      </td><td style=\"text-align: right;\">0.269306</td><td style=\"text-align: right;\">0.730694</td></tr>\n",
       "<tr><td>YES      </td><td style=\"text-align: right;\">0.269306</td><td style=\"text-align: right;\">0.730694</td></tr>\n",
       "<tr><td>YES      </td><td style=\"text-align: right;\">0.285171</td><td style=\"text-align: right;\">0.714829</td></tr>\n",
       "<tr><td>YES      </td><td style=\"text-align: right;\">0.259891</td><td style=\"text-align: right;\">0.740109</td></tr>\n",
       "<tr><td>YES      </td><td style=\"text-align: right;\">0.500777</td><td style=\"text-align: right;\">0.499223</td></tr>\n",
       "<tr><td>YES      </td><td style=\"text-align: right;\">0.514697</td><td style=\"text-align: right;\">0.485303</td></tr>\n",
       "<tr><td>YES      </td><td style=\"text-align: right;\">0.514697</td><td style=\"text-align: right;\">0.485303</td></tr>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_rf2.predict(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"DL Model\"></a>\n",
    "`Build Deep Learning`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "deeplearning Model Build Progress: [##################################################] 100%\n",
      "\n",
      "ModelMetricsBinomial: deeplearning\n",
      "** Reported on test data. **\n",
      "\n",
      "MSE: 0.22280968518794017\n",
      "R^2: 0.10737047406121802\n",
      "LogLoss: 0.6374853419025917\n",
      "AUC: 0.708553916874162\n",
      "Gini: 0.41710783374832405\n",
      "\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.25611323581363243: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>NO</b></td>\n",
       "<td><b>YES</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>NO</td>\n",
       "<td>1714.0</td>\n",
       "<td>3530.0</td>\n",
       "<td>0.6732</td>\n",
       "<td> (3530.0/5244.0)</td></tr>\n",
       "<tr><td>YES</td>\n",
       "<td>627.0</td>\n",
       "<td>5048.0</td>\n",
       "<td>0.1105</td>\n",
       "<td> (627.0/5675.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>2341.0</td>\n",
       "<td>8578.0</td>\n",
       "<td>0.3807</td>\n",
       "<td> (4157.0/10919.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       NO    YES    Error    Rate\n",
       "-----  ----  -----  -------  ----------------\n",
       "NO     1714  3530   0.6732   (3530.0/5244.0)\n",
       "YES    627   5048   0.1105   (627.0/5675.0)\n",
       "Total  2341  8578   0.3807   (4157.0/10919.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.2561132</td>\n",
       "<td>0.7083421</td>\n",
       "<td>315.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.0759424</td>\n",
       "<td>0.8445140</td>\n",
       "<td>391.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.4576310</td>\n",
       "<td>0.6688179</td>\n",
       "<td>214.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.4132508</td>\n",
       "<td>0.6553714</td>\n",
       "<td>237.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.9607276</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0476740</td>\n",
       "<td>1.0</td>\n",
       "<td>398.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.9607276</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_MCC</td>\n",
       "<td>0.4132508</td>\n",
       "<td>0.3090235</td>\n",
       "<td>237.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.4319226</td>\n",
       "<td>0.6519824</td>\n",
       "<td>227.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                      threshold    value     idx\n",
       "--------------------------  -----------  --------  -----\n",
       "max f1                      0.256113     0.708342  315\n",
       "max f2                      0.0759424    0.844514  391\n",
       "max f0point5                0.457631     0.668818  214\n",
       "max accuracy                0.413251     0.655371  237\n",
       "max precision               0.960728     1         0\n",
       "max recall                  0.047674     1         398\n",
       "max specificity             0.960728     1         0\n",
       "max absolute_MCC            0.413251     0.309023  237\n",
       "max min_per_class_accuracy  0.431923     0.651982  227"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gains/Lift Table: Avg response rate: 51.97 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0101658</td>\n",
       "<td>0.9026969</td>\n",
       "<td>1.8373838</td>\n",
       "<td>1.8373838</td>\n",
       "<td>0.9549550</td>\n",
       "<td>0.9549550</td>\n",
       "<td>0.0186784</td>\n",
       "<td>0.0186784</td>\n",
       "<td>83.7383816</td>\n",
       "<td>83.7383816</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0201484</td>\n",
       "<td>0.8678130</td>\n",
       "<td>1.6769268</td>\n",
       "<td>1.7578847</td>\n",
       "<td>0.8715596</td>\n",
       "<td>0.9136364</td>\n",
       "<td>0.0167401</td>\n",
       "<td>0.0354185</td>\n",
       "<td>67.6926808</td>\n",
       "<td>75.7884662</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0300394</td>\n",
       "<td>0.8432173</td>\n",
       "<td>1.6390080</td>\n",
       "<td>1.7187423</td>\n",
       "<td>0.8518519</td>\n",
       "<td>0.8932927</td>\n",
       "<td>0.0162115</td>\n",
       "<td>0.0516300</td>\n",
       "<td>63.9007995</td>\n",
       "<td>71.8742344</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0400220</td>\n",
       "<td>0.8207134</td>\n",
       "<td>1.6945787</td>\n",
       "<td>1.7127152</td>\n",
       "<td>0.8807339</td>\n",
       "<td>0.8901602</td>\n",
       "<td>0.0169163</td>\n",
       "<td>0.0685463</td>\n",
       "<td>69.4578669</td>\n",
       "<td>71.2715249</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0500046</td>\n",
       "<td>0.8009048</td>\n",
       "<td>1.3944970</td>\n",
       "<td>1.6491882</td>\n",
       "<td>0.7247706</td>\n",
       "<td>0.8571429</td>\n",
       "<td>0.0139207</td>\n",
       "<td>0.0824670</td>\n",
       "<td>39.4497029</td>\n",
       "<td>64.9188169</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1000092</td>\n",
       "<td>0.7365274</td>\n",
       "<td>1.4800407</td>\n",
       "<td>1.5646144</td>\n",
       "<td>0.7692308</td>\n",
       "<td>0.8131868</td>\n",
       "<td>0.0740088</td>\n",
       "<td>0.1564758</td>\n",
       "<td>48.0040664</td>\n",
       "<td>56.4614416</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1500137</td>\n",
       "<td>0.6847974</td>\n",
       "<td>1.4307060</td>\n",
       "<td>1.5199783</td>\n",
       "<td>0.7435897</td>\n",
       "<td>0.7899878</td>\n",
       "<td>0.0715419</td>\n",
       "<td>0.2280176</td>\n",
       "<td>43.0705975</td>\n",
       "<td>51.9978269</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2001099</td>\n",
       "<td>0.6468197</td>\n",
       "<td>1.3225665</td>\n",
       "<td>1.4705576</td>\n",
       "<td>0.6873857</td>\n",
       "<td>0.7643021</td>\n",
       "<td>0.0662555</td>\n",
       "<td>0.2942731</td>\n",
       "<td>32.2566502</td>\n",
       "<td>47.0557566</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3000275</td>\n",
       "<td>0.5703766</td>\n",
       "<td>1.3244397</td>\n",
       "<td>1.4218962</td>\n",
       "<td>0.6883593</td>\n",
       "<td>0.7390110</td>\n",
       "<td>0.1323348</td>\n",
       "<td>0.4266079</td>\n",
       "<td>32.4439689</td>\n",
       "<td>42.1896210</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4002198</td>\n",
       "<td>0.5007653</td>\n",
       "<td>1.1414171</td>\n",
       "<td>1.3516802</td>\n",
       "<td>0.5932358</td>\n",
       "<td>0.7025172</td>\n",
       "<td>0.1143612</td>\n",
       "<td>0.5409692</td>\n",
       "<td>14.1417101</td>\n",
       "<td>35.1680158</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5000458</td>\n",
       "<td>0.4324134</td>\n",
       "<td>1.0555813</td>\n",
       "<td>1.2925688</td>\n",
       "<td>0.5486239</td>\n",
       "<td>0.6717949</td>\n",
       "<td>0.1053744</td>\n",
       "<td>0.6463436</td>\n",
       "<td>5.5581296</td>\n",
       "<td>29.2568847</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.5999634</td>\n",
       "<td>0.3722628</td>\n",
       "<td>0.9664354</td>\n",
       "<td>1.2382548</td>\n",
       "<td>0.5022915</td>\n",
       "<td>0.6435659</td>\n",
       "<td>0.0965639</td>\n",
       "<td>0.7429075</td>\n",
       "<td>-3.3564648</td>\n",
       "<td>23.8254751</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.6999725</td>\n",
       "<td>0.3106081</td>\n",
       "<td>0.8439756</td>\n",
       "<td>1.1819218</td>\n",
       "<td>0.4386447</td>\n",
       "<td>0.6142876</td>\n",
       "<td>0.0844053</td>\n",
       "<td>0.8273128</td>\n",
       "<td>-15.6024431</td>\n",
       "<td>18.1921784</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.8000733</td>\n",
       "<td>0.2454952</td>\n",
       "<td>0.6900537</td>\n",
       "<td>1.1203820</td>\n",
       "<td>0.3586459</td>\n",
       "<td>0.5823031</td>\n",
       "<td>0.0690749</td>\n",
       "<td>0.8963877</td>\n",
       "<td>-30.9946274</td>\n",
       "<td>12.0381973</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.9000824</td>\n",
       "<td>0.1787110</td>\n",
       "<td>0.6008260</td>\n",
       "<td>1.0626535</td>\n",
       "<td>0.3122711</td>\n",
       "<td>0.5522996</td>\n",
       "<td>0.0600881</td>\n",
       "<td>0.9564758</td>\n",
       "<td>-39.9173968</td>\n",
       "<td>6.2653535</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0395820</td>\n",
       "<td>0.4356013</td>\n",
       "<td>1.0</td>\n",
       "<td>0.2263978</td>\n",
       "<td>0.5197362</td>\n",
       "<td>0.0435242</td>\n",
       "<td>1.0</td>\n",
       "<td>-56.4398664</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift      cumulative_lift    response_rate    cumulative_response_rate    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  --------  -----------------  ---------------  --------------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.0101658                   0.902697           1.83738   1.83738            0.954955         0.954955                    0.0186784       0.0186784                  83.7384   83.7384\n",
       "    2        0.0201484                   0.867813           1.67693   1.75788            0.87156          0.913636                    0.0167401       0.0354185                  67.6927   75.7885\n",
       "    3        0.0300394                   0.843217           1.63901   1.71874            0.851852         0.893293                    0.0162115       0.05163                    63.9008   71.8742\n",
       "    4        0.040022                    0.820713           1.69458   1.71272            0.880734         0.89016                     0.0169163       0.0685463                  69.4579   71.2715\n",
       "    5        0.0500046                   0.800905           1.3945    1.64919            0.724771         0.857143                    0.0139207       0.082467                   39.4497   64.9188\n",
       "    6        0.100009                    0.736527           1.48004   1.56461            0.769231         0.813187                    0.0740088       0.156476                   48.0041   56.4614\n",
       "    7        0.150014                    0.684797           1.43071   1.51998            0.74359          0.789988                    0.0715419       0.228018                   43.0706   51.9978\n",
       "    8        0.20011                     0.64682            1.32257   1.47056            0.687386         0.764302                    0.0662555       0.294273                   32.2567   47.0558\n",
       "    9        0.300027                    0.570377           1.32444   1.4219             0.688359         0.739011                    0.132335        0.426608                   32.444    42.1896\n",
       "    10       0.40022                     0.500765           1.14142   1.35168            0.593236         0.702517                    0.114361        0.540969                   14.1417   35.168\n",
       "    11       0.500046                    0.432413           1.05558   1.29257            0.548624         0.671795                    0.105374        0.646344                   5.55813   29.2569\n",
       "    12       0.599963                    0.372263           0.966435  1.23825            0.502291         0.643566                    0.0965639       0.742907                   -3.35646  23.8255\n",
       "    13       0.699973                    0.310608           0.843976  1.18192            0.438645         0.614288                    0.0844053       0.827313                   -15.6024  18.1922\n",
       "    14       0.800073                    0.245495           0.690054  1.12038            0.358646         0.582303                    0.0690749       0.896388                   -30.9946  12.0382\n",
       "    15       0.900082                    0.178711           0.600826  1.06265            0.312271         0.5523                      0.0600881       0.956476                   -39.9174  6.26535\n",
       "    16       1                           0.039582           0.435601  1                  0.226398         0.519736                    0.0435242       1                          -56.4399  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set response column\n",
    "myY = \"IsDepDelayed\"\n",
    "# Set feature columns\n",
    "myX = [\"Origin\", \"Dest\", \"Year\", \"UniqueCarrier\", \"DayOfWeek\", \"Month\", \"Distance\", \"FlightNum\"]\n",
    "\n",
    "# Predict delays with Deep Learning\n",
    "data_dl = H2ODeepLearningEstimator(hidden = [10,10], epochs = 5, variable_importances = True,\n",
    "                                   balance_classes = False, loss = \"Automatic\")\n",
    "data_dl.train(x = myX, y = myY, training_frame = train, validation_frame=test)\n",
    "\n",
    "data_dl.model_performance(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "deeplearning prediction Progress: [##################################################] 100%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>predict  </th><th style=\"text-align: right;\">      NO</th><th style=\"text-align: right;\">     YES</th></tr>\n",
       "<tr><td>YES      </td><td style=\"text-align: right;\">0.623984</td><td style=\"text-align: right;\">0.376016</td></tr>\n",
       "<tr><td>YES      </td><td style=\"text-align: right;\">0.550361</td><td style=\"text-align: right;\">0.449639</td></tr>\n",
       "<tr><td>YES      </td><td style=\"text-align: right;\">0.402294</td><td style=\"text-align: right;\">0.597706</td></tr>\n",
       "<tr><td>YES      </td><td style=\"text-align: right;\">0.566555</td><td style=\"text-align: right;\">0.433445</td></tr>\n",
       "<tr><td>YES      </td><td style=\"text-align: right;\">0.528912</td><td style=\"text-align: right;\">0.471088</td></tr>\n",
       "<tr><td>YES      </td><td style=\"text-align: right;\">0.507642</td><td style=\"text-align: right;\">0.492358</td></tr>\n",
       "<tr><td>YES      </td><td style=\"text-align: right;\">0.442312</td><td style=\"text-align: right;\">0.557688</td></tr>\n",
       "<tr><td>YES      </td><td style=\"text-align: right;\">0.47574 </td><td style=\"text-align: right;\">0.52426 </td></tr>\n",
       "<tr><td>YES      </td><td style=\"text-align: right;\">0.518665</td><td style=\"text-align: right;\">0.481335</td></tr>\n",
       "<tr><td>YES      </td><td style=\"text-align: right;\">0.489721</td><td style=\"text-align: right;\">0.510279</td></tr>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dl.predict(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"NB Model\"></a>\n",
    "`Build Naive Bayes`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "naivebayes Model Build Progress: [##################################################] 100%\n",
      "\n",
      "ModelMetricsBinomial: naivebayes\n",
      "** Reported on test data. **\n",
      "\n",
      "MSE: 0.23152996070216778\n",
      "R^2: 0.07243493976541415\n",
      "LogLoss: 0.6647084512439999\n",
      "AUC: 0.6877895778519272\n",
      "Gini: 0.37557915570385436\n",
      "\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.22770709644158957: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>NO</b></td>\n",
       "<td><b>YES</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>NO</td>\n",
       "<td>1199.0</td>\n",
       "<td>4045.0</td>\n",
       "<td>0.7714</td>\n",
       "<td> (4045.0/5244.0)</td></tr>\n",
       "<tr><td>YES</td>\n",
       "<td>425.0</td>\n",
       "<td>5250.0</td>\n",
       "<td>0.0749</td>\n",
       "<td> (425.0/5675.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>1624.0</td>\n",
       "<td>9295.0</td>\n",
       "<td>0.4094</td>\n",
       "<td> (4470.0/10919.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       NO    YES    Error    Rate\n",
       "-----  ----  -----  -------  ----------------\n",
       "NO     1199  4045   0.7714   (4045.0/5244.0)\n",
       "YES    425   5250   0.0749   (425.0/5675.0)\n",
       "Total  1624  9295   0.4094   (4470.0/10919.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.2277071</td>\n",
       "<td>0.7014028</td>\n",
       "<td>334.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.0639758</td>\n",
       "<td>0.8453940</td>\n",
       "<td>390.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.5630666</td>\n",
       "<td>0.6520698</td>\n",
       "<td>188.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.4959973</td>\n",
       "<td>0.6370547</td>\n",
       "<td>218.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.9974487</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0207270</td>\n",
       "<td>1.0</td>\n",
       "<td>398.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.9974487</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_MCC</td>\n",
       "<td>0.5630666</td>\n",
       "<td>0.2765067</td>\n",
       "<td>188.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.4985902</td>\n",
       "<td>0.6366520</td>\n",
       "<td>217.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                      threshold    value     idx\n",
       "--------------------------  -----------  --------  -----\n",
       "max f1                      0.227707     0.701403  334\n",
       "max f2                      0.0639758    0.845394  390\n",
       "max f0point5                0.563067     0.65207   188\n",
       "max accuracy                0.495997     0.637055  218\n",
       "max precision               0.997449     1         0\n",
       "max recall                  0.020727     1         398\n",
       "max specificity             0.997449     1         0\n",
       "max absolute_MCC            0.563067     0.276507  188\n",
       "max min_per_class_accuracy  0.49859      0.636652  217"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gains/Lift Table: Avg response rate: 51.97 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0100742</td>\n",
       "<td>0.9849015</td>\n",
       "<td>1.8365959</td>\n",
       "<td>1.8365959</td>\n",
       "<td>0.9545455</td>\n",
       "<td>0.9545455</td>\n",
       "<td>0.0185022</td>\n",
       "<td>0.0185022</td>\n",
       "<td>83.6595915</td>\n",
       "<td>83.6595915</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0200568</td>\n",
       "<td>0.9736685</td>\n",
       "<td>1.6945787</td>\n",
       "<td>1.7659115</td>\n",
       "<td>0.8807339</td>\n",
       "<td>0.9178082</td>\n",
       "<td>0.0169163</td>\n",
       "<td>0.0354185</td>\n",
       "<td>69.4578669</td>\n",
       "<td>76.5911532</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0300394</td>\n",
       "<td>0.9599325</td>\n",
       "<td>1.5533638</td>\n",
       "<td>1.6952783</td>\n",
       "<td>0.8073394</td>\n",
       "<td>0.8810976</td>\n",
       "<td>0.0155066</td>\n",
       "<td>0.0509251</td>\n",
       "<td>55.3363780</td>\n",
       "<td>69.5278285</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0400220</td>\n",
       "<td>0.9461100</td>\n",
       "<td>1.4651045</td>\n",
       "<td>1.6378665</td>\n",
       "<td>0.7614679</td>\n",
       "<td>0.8512586</td>\n",
       "<td>0.0146256</td>\n",
       "<td>0.0655507</td>\n",
       "<td>46.5104474</td>\n",
       "<td>63.7866511</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0500046</td>\n",
       "<td>0.9340485</td>\n",
       "<td>1.5533638</td>\n",
       "<td>1.6209969</td>\n",
       "<td>0.8073394</td>\n",
       "<td>0.8424908</td>\n",
       "<td>0.0155066</td>\n",
       "<td>0.0810573</td>\n",
       "<td>55.3363780</td>\n",
       "<td>62.0996918</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1000092</td>\n",
       "<td>0.8714906</td>\n",
       "<td>1.5328993</td>\n",
       "<td>1.5769481</td>\n",
       "<td>0.7967033</td>\n",
       "<td>0.8195971</td>\n",
       "<td>0.0766520</td>\n",
       "<td>0.1577093</td>\n",
       "<td>53.2899259</td>\n",
       "<td>57.6948089</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1501053</td>\n",
       "<td>0.8159428</td>\n",
       "<td>1.3682935</td>\n",
       "<td>1.5073117</td>\n",
       "<td>0.7111517</td>\n",
       "<td>0.7834045</td>\n",
       "<td>0.0685463</td>\n",
       "<td>0.2262555</td>\n",
       "<td>36.8293535</td>\n",
       "<td>50.7311700</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2001099</td>\n",
       "<td>0.7705500</td>\n",
       "<td>1.3743235</td>\n",
       "<td>1.4740799</td>\n",
       "<td>0.7142857</td>\n",
       "<td>0.7661327</td>\n",
       "<td>0.0687225</td>\n",
       "<td>0.2949780</td>\n",
       "<td>37.4323474</td>\n",
       "<td>47.4079860</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3001191</td>\n",
       "<td>0.6889571</td>\n",
       "<td>1.2227955</td>\n",
       "<td>1.3903440</td>\n",
       "<td>0.6355311</td>\n",
       "<td>0.7226121</td>\n",
       "<td>0.1222907</td>\n",
       "<td>0.4172687</td>\n",
       "<td>22.2795501</td>\n",
       "<td>39.0343967</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4001282</td>\n",
       "<td>0.6054809</td>\n",
       "<td>1.0941729</td>\n",
       "<td>1.3163182</td>\n",
       "<td>0.5686813</td>\n",
       "<td>0.6841382</td>\n",
       "<td>0.1094273</td>\n",
       "<td>0.5266960</td>\n",
       "<td>9.4172920</td>\n",
       "<td>31.6318153</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5001374</td>\n",
       "<td>0.5012963</td>\n",
       "<td>1.0413143</td>\n",
       "<td>1.2613275</td>\n",
       "<td>0.5412088</td>\n",
       "<td>0.6555576</td>\n",
       "<td>0.1041410</td>\n",
       "<td>0.6308370</td>\n",
       "<td>4.1314324</td>\n",
       "<td>26.1327459</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.5999634</td>\n",
       "<td>0.4084338</td>\n",
       "<td>0.8772975</td>\n",
       "<td>1.1974299</td>\n",
       "<td>0.4559633</td>\n",
       "<td>0.6223477</td>\n",
       "<td>0.0875771</td>\n",
       "<td>0.7184141</td>\n",
       "<td>-12.2702502</td>\n",
       "<td>19.7429938</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.6999725</td>\n",
       "<td>0.3306943</td>\n",
       "<td>0.8545473</td>\n",
       "<td>1.1484403</td>\n",
       "<td>0.4441392</td>\n",
       "<td>0.5968860</td>\n",
       "<td>0.0854626</td>\n",
       "<td>0.8038767</td>\n",
       "<td>-14.5452712</td>\n",
       "<td>14.8440293</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.7999817</td>\n",
       "<td>0.2637767</td>\n",
       "<td>0.8175463</td>\n",
       "<td>1.1070738</td>\n",
       "<td>0.4249084</td>\n",
       "<td>0.5753864</td>\n",
       "<td>0.0817621</td>\n",
       "<td>0.8856388</td>\n",
       "<td>-18.2453728</td>\n",
       "<td>10.7073806</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.8999908</td>\n",
       "<td>0.1810967</td>\n",
       "<td>0.6624944</td>\n",
       "<td>1.0576711</td>\n",
       "<td>0.3443223</td>\n",
       "<td>0.5497100</td>\n",
       "<td>0.0662555</td>\n",
       "<td>0.9518943</td>\n",
       "<td>-33.7505607</td>\n",
       "<td>5.7671066</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0126029</td>\n",
       "<td>0.4810132</td>\n",
       "<td>1.0</td>\n",
       "<td>0.25</td>\n",
       "<td>0.5197362</td>\n",
       "<td>0.0481057</td>\n",
       "<td>1.0</td>\n",
       "<td>-51.8986784</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift      cumulative_lift    response_rate    cumulative_response_rate    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  --------  -----------------  ---------------  --------------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.0100742                   0.984901           1.8366    1.8366             0.954545         0.954545                    0.0185022       0.0185022                  83.6596   83.6596\n",
       "    2        0.0200568                   0.973668           1.69458   1.76591            0.880734         0.917808                    0.0169163       0.0354185                  69.4579   76.5912\n",
       "    3        0.0300394                   0.959933           1.55336   1.69528            0.807339         0.881098                    0.0155066       0.0509251                  55.3364   69.5278\n",
       "    4        0.040022                    0.94611            1.4651    1.63787            0.761468         0.851259                    0.0146256       0.0655507                  46.5104   63.7867\n",
       "    5        0.0500046                   0.934049           1.55336   1.621              0.807339         0.842491                    0.0155066       0.0810573                  55.3364   62.0997\n",
       "    6        0.100009                    0.871491           1.5329    1.57695            0.796703         0.819597                    0.076652        0.157709                   53.2899   57.6948\n",
       "    7        0.150105                    0.815943           1.36829   1.50731            0.711152         0.783405                    0.0685463       0.226256                   36.8294   50.7312\n",
       "    8        0.20011                     0.77055            1.37432   1.47408            0.714286         0.766133                    0.0687225       0.294978                   37.4323   47.408\n",
       "    9        0.300119                    0.688957           1.2228    1.39034            0.635531         0.722612                    0.122291        0.417269                   22.2796   39.0344\n",
       "    10       0.400128                    0.605481           1.09417   1.31632            0.568681         0.684138                    0.109427        0.526696                   9.41729   31.6318\n",
       "    11       0.500137                    0.501296           1.04131   1.26133            0.541209         0.655558                    0.104141        0.630837                   4.13143   26.1327\n",
       "    12       0.599963                    0.408434           0.877297  1.19743            0.455963         0.622348                    0.0875771       0.718414                   -12.2703  19.743\n",
       "    13       0.699973                    0.330694           0.854547  1.14844            0.444139         0.596886                    0.0854626       0.803877                   -14.5453  14.844\n",
       "    14       0.799982                    0.263777           0.817546  1.10707            0.424908         0.575386                    0.0817621       0.885639                   -18.2454  10.7074\n",
       "    15       0.899991                    0.181097           0.662494  1.05767            0.344322         0.54971                     0.0662555       0.951894                   -33.7506  5.76711\n",
       "    16       1                           0.0126029          0.481013  1                  0.25             0.519736                    0.0481057       1                          -51.8987  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set response column\n",
    "myY = \"IsDepDelayed\"\n",
    "# Set feature columns\n",
    "myX = [\"Origin\", \"Dest\", \"Year\", \"UniqueCarrier\", \"DayOfWeek\", \"Month\", \"Distance\", \"FlightNum\"]\n",
    "\n",
    "# Predict delays with Naive Bayes\n",
    "# If laplace smoothing is disabled ('laplace=0') the algorithm will predict 0\n",
    "data_nb = H2ONaiveBayesEstimator(laplace=1) \n",
    "data_nb.train(x = myX, y = myY, training_frame = train, validation_frame=test)\n",
    "\n",
    "data_nb.model_performance(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "naivebayes prediction Progress: [##################################################] 100%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>predict  </th><th style=\"text-align: right;\">      NO</th><th style=\"text-align: right;\">     YES</th></tr>\n",
       "<tr><td>YES      </td><td style=\"text-align: right;\">0.396596</td><td style=\"text-align: right;\">0.603404</td></tr>\n",
       "<tr><td>YES      </td><td style=\"text-align: right;\">0.307507</td><td style=\"text-align: right;\">0.692493</td></tr>\n",
       "<tr><td>YES      </td><td style=\"text-align: right;\">0.260807</td><td style=\"text-align: right;\">0.739193</td></tr>\n",
       "<tr><td>YES      </td><td style=\"text-align: right;\">0.296727</td><td style=\"text-align: right;\">0.703273</td></tr>\n",
       "<tr><td>YES      </td><td style=\"text-align: right;\">0.285976</td><td style=\"text-align: right;\">0.714024</td></tr>\n",
       "<tr><td>YES      </td><td style=\"text-align: right;\">0.333018</td><td style=\"text-align: right;\">0.666982</td></tr>\n",
       "<tr><td>YES      </td><td style=\"text-align: right;\">0.221825</td><td style=\"text-align: right;\">0.778175</td></tr>\n",
       "<tr><td>YES      </td><td style=\"text-align: right;\">0.498704</td><td style=\"text-align: right;\">0.501296</td></tr>\n",
       "<tr><td>YES      </td><td style=\"text-align: right;\">0.509004</td><td style=\"text-align: right;\">0.490996</td></tr>\n",
       "<tr><td>YES      </td><td style=\"text-align: right;\">0.495986</td><td style=\"text-align: right;\">0.504014</td></tr>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_nb.predict(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"output\"></a>\n",
    "## Model Performance and Output \n",
    "\n",
    "Run each cell below to see each model's performance on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ModelMetricsBinomialGLM: glm\n",
      "** Reported on test data. **\n",
      "\n",
      "MSE: 0.21416530940414985\n",
      "R^2: 0.14200193566672592\n",
      "LogLoss: 0.616556553874648\n",
      "Null degrees of freedom: 10918\n",
      "Residual degrees of freedom: 10678\n",
      "Null deviance: 15122.125326794045\n",
      "Residual deviance: 13464.362023514564\n",
      "AIC: 13946.362023514564\n",
      "AUC: 0.7166694556732763\n",
      "Gini: 0.43333891134655267\n",
      "\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.331854947054549: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>NO</b></td>\n",
       "<td><b>YES</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>NO</td>\n",
       "<td>1496.0</td>\n",
       "<td>3748.0</td>\n",
       "<td>0.7147</td>\n",
       "<td> (3748.0/5244.0)</td></tr>\n",
       "<tr><td>YES</td>\n",
       "<td>505.0</td>\n",
       "<td>5170.0</td>\n",
       "<td>0.089</td>\n",
       "<td> (505.0/5675.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>2001.0</td>\n",
       "<td>8918.0</td>\n",
       "<td>0.3895</td>\n",
       "<td> (4253.0/10919.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       NO    YES    Error    Rate\n",
       "-----  ----  -----  -------  ----------------\n",
       "NO     1496  3748   0.7147   (3748.0/5244.0)\n",
       "YES    505   5170   0.089    (505.0/5675.0)\n",
       "Total  2001  8918   0.3895   (4253.0/10919.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.3318549</td>\n",
       "<td>0.7085589</td>\n",
       "<td>314.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.1648148</td>\n",
       "<td>0.8451715</td>\n",
       "<td>379.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.5324188</td>\n",
       "<td>0.6755473</td>\n",
       "<td>203.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.5097744</td>\n",
       "<td>0.6633391</td>\n",
       "<td>216.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.9797664</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0806012</td>\n",
       "<td>1.0</td>\n",
       "<td>399.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.9797664</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_MCC</td>\n",
       "<td>0.5097744</td>\n",
       "<td>0.3251416</td>\n",
       "<td>216.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.5269278</td>\n",
       "<td>0.6601831</td>\n",
       "<td>207.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                      threshold    value     idx\n",
       "--------------------------  -----------  --------  -----\n",
       "max f1                      0.331855     0.708559  314\n",
       "max f2                      0.164815     0.845172  379\n",
       "max f0point5                0.532419     0.675547  203\n",
       "max accuracy                0.509774     0.663339  216\n",
       "max precision               0.979766     1         0\n",
       "max recall                  0.0806012    1         399\n",
       "max specificity             0.979766     1         0\n",
       "max absolute_MCC            0.509774     0.325142  216\n",
       "max min_per_class_accuracy  0.526928     0.660183  207"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gains/Lift Table: Avg response rate: 51.97 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0100742</td>\n",
       "<td>0.9115665</td>\n",
       "<td>1.8540873</td>\n",
       "<td>1.8540873</td>\n",
       "<td>0.9636364</td>\n",
       "<td>0.9636364</td>\n",
       "<td>0.0186784</td>\n",
       "<td>0.0186784</td>\n",
       "<td>85.4087305</td>\n",
       "<td>85.4087305</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0200568</td>\n",
       "<td>0.8818044</td>\n",
       "<td>1.6239712</td>\n",
       "<td>1.7395546</td>\n",
       "<td>0.8440367</td>\n",
       "<td>0.9041096</td>\n",
       "<td>0.0162115</td>\n",
       "<td>0.0348899</td>\n",
       "<td>62.3971224</td>\n",
       "<td>73.9554644</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0300394</td>\n",
       "<td>0.8647063</td>\n",
       "<td>1.7828380</td>\n",
       "<td>1.7539384</td>\n",
       "<td>0.9266055</td>\n",
       "<td>0.9115854</td>\n",
       "<td>0.0177974</td>\n",
       "<td>0.0526872</td>\n",
       "<td>78.2837974</td>\n",
       "<td>75.3938433</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0400220</td>\n",
       "<td>0.8536446</td>\n",
       "<td>1.6769268</td>\n",
       "<td>1.7347296</td>\n",
       "<td>0.8715596</td>\n",
       "<td>0.9016018</td>\n",
       "<td>0.0167401</td>\n",
       "<td>0.0694273</td>\n",
       "<td>67.6926808</td>\n",
       "<td>73.4729584</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0500046</td>\n",
       "<td>0.8431967</td>\n",
       "<td>1.5180601</td>\n",
       "<td>1.6914750</td>\n",
       "<td>0.7889908</td>\n",
       "<td>0.8791209</td>\n",
       "<td>0.0151542</td>\n",
       "<td>0.0845815</td>\n",
       "<td>51.8060057</td>\n",
       "<td>69.1475045</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1000092</td>\n",
       "<td>0.7799468</td>\n",
       "<td>1.5364232</td>\n",
       "<td>1.6139491</td>\n",
       "<td>0.7985348</td>\n",
       "<td>0.8388278</td>\n",
       "<td>0.0768282</td>\n",
       "<td>0.1614097</td>\n",
       "<td>53.6423166</td>\n",
       "<td>61.3949105</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1500137</td>\n",
       "<td>0.7336259</td>\n",
       "<td>1.4694689</td>\n",
       "<td>1.5657891</td>\n",
       "<td>0.7637363</td>\n",
       "<td>0.8137973</td>\n",
       "<td>0.0734802</td>\n",
       "<td>0.2348899</td>\n",
       "<td>46.9468945</td>\n",
       "<td>56.5789052</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2001099</td>\n",
       "<td>0.6998976</td>\n",
       "<td>1.3120141</td>\n",
       "<td>1.5022582</td>\n",
       "<td>0.6819013</td>\n",
       "<td>0.7807780</td>\n",
       "<td>0.0657269</td>\n",
       "<td>0.3006167</td>\n",
       "<td>31.2014110</td>\n",
       "<td>50.2258208</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3000275</td>\n",
       "<td>0.6405624</td>\n",
       "<td>1.2662419</td>\n",
       "<td>1.4236582</td>\n",
       "<td>0.6581118</td>\n",
       "<td>0.7399267</td>\n",
       "<td>0.1265198</td>\n",
       "<td>0.4271366</td>\n",
       "<td>26.6241939</td>\n",
       "<td>42.3658163</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4000366</td>\n",
       "<td>0.5861789</td>\n",
       "<td>1.2245575</td>\n",
       "<td>1.3738830</td>\n",
       "<td>0.6364469</td>\n",
       "<td>0.7140568</td>\n",
       "<td>0.1224670</td>\n",
       "<td>0.5496035</td>\n",
       "<td>22.4557454</td>\n",
       "<td>37.3882986</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5000458</td>\n",
       "<td>0.5303817</td>\n",
       "<td>1.0501241</td>\n",
       "<td>1.3091312</td>\n",
       "<td>0.5457875</td>\n",
       "<td>0.6804029</td>\n",
       "<td>0.1050220</td>\n",
       "<td>0.6546256</td>\n",
       "<td>5.0124090</td>\n",
       "<td>30.9131207</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.5999634</td>\n",
       "<td>0.4603878</td>\n",
       "<td>0.9470361</td>\n",
       "<td>1.2488281</td>\n",
       "<td>0.4922090</td>\n",
       "<td>0.6490612</td>\n",
       "<td>0.0946256</td>\n",
       "<td>0.7492511</td>\n",
       "<td>-5.2963898</td>\n",
       "<td>24.8828084</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.6999725</td>\n",
       "<td>0.4027967</td>\n",
       "<td>0.8087365</td>\n",
       "<td>1.1859496</td>\n",
       "<td>0.4203297</td>\n",
       "<td>0.6163810</td>\n",
       "<td>0.0808811</td>\n",
       "<td>0.8301322</td>\n",
       "<td>-19.1263494</td>\n",
       "<td>18.5949632</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.7999817</td>\n",
       "<td>0.3422599</td>\n",
       "<td>0.6889237</td>\n",
       "<td>1.1238143</td>\n",
       "<td>0.3580586</td>\n",
       "<td>0.5840870</td>\n",
       "<td>0.0688987</td>\n",
       "<td>0.8990308</td>\n",
       "<td>-31.1076310</td>\n",
       "<td>12.3814277</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.8999908</td>\n",
       "<td>0.2686606</td>\n",
       "<td>0.5796826</td>\n",
       "<td>1.0633490</td>\n",
       "<td>0.3012821</td>\n",
       "<td>0.5526610</td>\n",
       "<td>0.0579736</td>\n",
       "<td>0.9570044</td>\n",
       "<td>-42.0317407</td>\n",
       "<td>6.3349049</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0797865</td>\n",
       "<td>0.4299166</td>\n",
       "<td>1.0</td>\n",
       "<td>0.2234432</td>\n",
       "<td>0.5197362</td>\n",
       "<td>0.0429956</td>\n",
       "<td>1.0</td>\n",
       "<td>-57.0083426</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift      cumulative_lift    response_rate    cumulative_response_rate    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  --------  -----------------  ---------------  --------------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.0100742                   0.911567           1.85409   1.85409            0.963636         0.963636                    0.0186784       0.0186784                  85.4087   85.4087\n",
       "    2        0.0200568                   0.881804           1.62397   1.73955            0.844037         0.90411                     0.0162115       0.0348899                  62.3971   73.9555\n",
       "    3        0.0300394                   0.864706           1.78284   1.75394            0.926606         0.911585                    0.0177974       0.0526872                  78.2838   75.3938\n",
       "    4        0.040022                    0.853645           1.67693   1.73473            0.87156          0.901602                    0.0167401       0.0694273                  67.6927   73.473\n",
       "    5        0.0500046                   0.843197           1.51806   1.69148            0.788991         0.879121                    0.0151542       0.0845815                  51.806    69.1475\n",
       "    6        0.100009                    0.779947           1.53642   1.61395            0.798535         0.838828                    0.0768282       0.16141                    53.6423   61.3949\n",
       "    7        0.150014                    0.733626           1.46947   1.56579            0.763736         0.813797                    0.0734802       0.23489                    46.9469   56.5789\n",
       "    8        0.20011                     0.699898           1.31201   1.50226            0.681901         0.780778                    0.0657269       0.300617                   31.2014   50.2258\n",
       "    9        0.300027                    0.640562           1.26624   1.42366            0.658112         0.739927                    0.12652         0.427137                   26.6242   42.3658\n",
       "    10       0.400037                    0.586179           1.22456   1.37388            0.636447         0.714057                    0.122467        0.549604                   22.4557   37.3883\n",
       "    11       0.500046                    0.530382           1.05012   1.30913            0.545788         0.680403                    0.105022        0.654626                   5.01241   30.9131\n",
       "    12       0.599963                    0.460388           0.947036  1.24883            0.492209         0.649061                    0.0946256       0.749251                   -5.29639  24.8828\n",
       "    13       0.699973                    0.402797           0.808737  1.18595            0.42033          0.616381                    0.0808811       0.830132                   -19.1263  18.595\n",
       "    14       0.799982                    0.34226            0.688924  1.12381            0.358059         0.584087                    0.0688987       0.899031                   -31.1076  12.3814\n",
       "    15       0.899991                    0.268661           0.579683  1.06335            0.301282         0.552661                    0.0579736       0.957004                   -42.0317  6.3349\n",
       "    16       1                           0.0797865          0.429917  1                  0.223443         0.519736                    0.0429956       1                          -57.0083  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GLM performance\n",
    "data_glm.model_performance(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ModelMetricsBinomial: drf\n",
      "** Reported on test data. **\n",
      "\n",
      "MSE: 0.21599360206518958\n",
      "R^2: 0.1346773527612536\n",
      "LogLoss: 0.6214047527338338\n",
      "AUC: 0.7177846214847596\n",
      "Gini: 0.43556924296951927\n",
      "\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.4300728998567959: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>NO</b></td>\n",
       "<td><b>YES</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>NO</td>\n",
       "<td>2368.0</td>\n",
       "<td>2876.0</td>\n",
       "<td>0.5484</td>\n",
       "<td> (2876.0/5244.0)</td></tr>\n",
       "<tr><td>YES</td>\n",
       "<td>985.0</td>\n",
       "<td>4690.0</td>\n",
       "<td>0.1736</td>\n",
       "<td> (985.0/5675.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>3353.0</td>\n",
       "<td>7566.0</td>\n",
       "<td>0.3536</td>\n",
       "<td> (3861.0/10919.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       NO    YES    Error    Rate\n",
       "-----  ----  -----  -------  ----------------\n",
       "NO     2368  2876   0.5484   (2876.0/5244.0)\n",
       "YES    985   4690   0.1736   (985.0/5675.0)\n",
       "Total  3353  7566   0.3536   (3861.0/10919.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.4300729</td>\n",
       "<td>0.7084057</td>\n",
       "<td>301.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.3002093</td>\n",
       "<td>0.8444856</td>\n",
       "<td>386.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.5293900</td>\n",
       "<td>0.6774533</td>\n",
       "<td>232.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.4886548</td>\n",
       "<td>0.6656287</td>\n",
       "<td>264.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.8944711</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.2455635</td>\n",
       "<td>1.0</td>\n",
       "<td>398.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.8944711</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_MCC</td>\n",
       "<td>0.4886548</td>\n",
       "<td>0.3292966</td>\n",
       "<td>264.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.5210644</td>\n",
       "<td>0.6616740</td>\n",
       "<td>238.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                      threshold    value     idx\n",
       "--------------------------  -----------  --------  -----\n",
       "max f1                      0.430073     0.708406  301\n",
       "max f2                      0.300209     0.844486  386\n",
       "max f0point5                0.52939      0.677453  232\n",
       "max accuracy                0.488655     0.665629  264\n",
       "max precision               0.894471     1         0\n",
       "max recall                  0.245563     1         398\n",
       "max specificity             0.894471     1         0\n",
       "max absolute_MCC            0.488655     0.329297  264\n",
       "max min_per_class_accuracy  0.521064     0.661674  238"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gains/Lift Table: Avg response rate: 51.97 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0100742</td>\n",
       "<td>0.8454385</td>\n",
       "<td>1.8016131</td>\n",
       "<td>1.8016131</td>\n",
       "<td>0.9363636</td>\n",
       "<td>0.9363636</td>\n",
       "<td>0.0181498</td>\n",
       "<td>0.0181498</td>\n",
       "<td>80.1613136</td>\n",
       "<td>80.1613136</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0202399</td>\n",
       "<td>0.8320967</td>\n",
       "<td>1.8373838</td>\n",
       "<td>1.8195794</td>\n",
       "<td>0.9549550</td>\n",
       "<td>0.9457014</td>\n",
       "<td>0.0186784</td>\n",
       "<td>0.0368282</td>\n",
       "<td>83.7383816</td>\n",
       "<td>81.9579405</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0303141</td>\n",
       "<td>0.8236593</td>\n",
       "<td>1.6092078</td>\n",
       "<td>1.7496674</td>\n",
       "<td>0.8363636</td>\n",
       "<td>0.9093656</td>\n",
       "<td>0.0162115</td>\n",
       "<td>0.0530396</td>\n",
       "<td>60.9207849</td>\n",
       "<td>74.9667408</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0401136</td>\n",
       "<td>0.8111829</td>\n",
       "<td>1.7082712</td>\n",
       "<td>1.7395546</td>\n",
       "<td>0.8878505</td>\n",
       "<td>0.9041096</td>\n",
       "<td>0.0167401</td>\n",
       "<td>0.0697797</td>\n",
       "<td>70.8271234</td>\n",
       "<td>73.9554644</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0500046</td>\n",
       "<td>0.8015050</td>\n",
       "<td>1.6033774</td>\n",
       "<td>1.7126185</td>\n",
       "<td>0.8333333</td>\n",
       "<td>0.8901099</td>\n",
       "<td>0.0158590</td>\n",
       "<td>0.0856388</td>\n",
       "<td>60.3377386</td>\n",
       "<td>71.2618483</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1001007</td>\n",
       "<td>0.7491098</td>\n",
       "<td>1.5300969</td>\n",
       "<td>1.6212742</td>\n",
       "<td>0.7952468</td>\n",
       "<td>0.8426349</td>\n",
       "<td>0.0766520</td>\n",
       "<td>0.1622907</td>\n",
       "<td>53.0096884</td>\n",
       "<td>62.1274188</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1501053</td>\n",
       "<td>0.7138253</td>\n",
       "<td>1.4659450</td>\n",
       "<td>1.5695294</td>\n",
       "<td>0.7619048</td>\n",
       "<td>0.8157413</td>\n",
       "<td>0.0733040</td>\n",
       "<td>0.2355947</td>\n",
       "<td>46.5945039</td>\n",
       "<td>56.9529395</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2001099</td>\n",
       "<td>0.6809330</td>\n",
       "<td>1.3884191</td>\n",
       "<td>1.5242725</td>\n",
       "<td>0.7216117</td>\n",
       "<td>0.7922197</td>\n",
       "<td>0.0694273</td>\n",
       "<td>0.3050220</td>\n",
       "<td>38.8419099</td>\n",
       "<td>52.4272543</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3000275</td>\n",
       "<td>0.6356334</td>\n",
       "<td>1.2609512</td>\n",
       "<td>1.4365792</td>\n",
       "<td>0.6553621</td>\n",
       "<td>0.7466422</td>\n",
       "<td>0.1259912</td>\n",
       "<td>0.4310132</td>\n",
       "<td>26.0951235</td>\n",
       "<td>43.6579153</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4000366</td>\n",
       "<td>0.5751088</td>\n",
       "<td>1.1681750</td>\n",
       "<td>1.3694781</td>\n",
       "<td>0.6071429</td>\n",
       "<td>0.7117674</td>\n",
       "<td>0.1168282</td>\n",
       "<td>0.5478414</td>\n",
       "<td>16.8174953</td>\n",
       "<td>36.9478103</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5042586</td>\n",
       "<td>0.5210011</td>\n",
       "<td>1.0837591</td>\n",
       "<td>1.3104247</td>\n",
       "<td>0.5632689</td>\n",
       "<td>0.6810752</td>\n",
       "<td>0.1129515</td>\n",
       "<td>0.6607930</td>\n",
       "<td>8.3759126</td>\n",
       "<td>31.0424671</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.6003297</td>\n",
       "<td>0.4725114</td>\n",
       "<td>0.9335967</td>\n",
       "<td>1.2501207</td>\n",
       "<td>0.4852240</td>\n",
       "<td>0.6497330</td>\n",
       "<td>0.0896916</td>\n",
       "<td>0.7504846</td>\n",
       "<td>-6.6403329</td>\n",
       "<td>25.0120693</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.7010715</td>\n",
       "<td>0.4275092</td>\n",
       "<td>0.7941091</td>\n",
       "<td>1.1845932</td>\n",
       "<td>0.4127273</td>\n",
       "<td>0.6156760</td>\n",
       "<td>0.08</td>\n",
       "<td>0.8304846</td>\n",
       "<td>-20.5890909</td>\n",
       "<td>18.4593226</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.8003480</td>\n",
       "<td>0.3929065</td>\n",
       "<td>0.6691586</td>\n",
       "<td>1.1206579</td>\n",
       "<td>0.3477860</td>\n",
       "<td>0.5824465</td>\n",
       "<td>0.0664317</td>\n",
       "<td>0.8969163</td>\n",
       "<td>-33.0841393</td>\n",
       "<td>12.0657864</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.9001740</td>\n",
       "<td>0.3614699</td>\n",
       "<td>0.5842766</td>\n",
       "<td>1.0611752</td>\n",
       "<td>0.3036697</td>\n",
       "<td>0.5515312</td>\n",
       "<td>0.0583260</td>\n",
       "<td>0.9552423</td>\n",
       "<td>-41.5723397</td>\n",
       "<td>6.1175152</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.2313936</td>\n",
       "<td>0.4483573</td>\n",
       "<td>1.0</td>\n",
       "<td>0.2330275</td>\n",
       "<td>0.5197362</td>\n",
       "<td>0.0447577</td>\n",
       "<td>1.0</td>\n",
       "<td>-55.1642727</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift      cumulative_lift    response_rate    cumulative_response_rate    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  --------  -----------------  ---------------  --------------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.0100742                   0.845438           1.80161   1.80161            0.936364         0.936364                    0.0181498       0.0181498                  80.1613   80.1613\n",
       "    2        0.0202399                   0.832097           1.83738   1.81958            0.954955         0.945701                    0.0186784       0.0368282                  83.7384   81.9579\n",
       "    3        0.0303141                   0.823659           1.60921   1.74967            0.836364         0.909366                    0.0162115       0.0530396                  60.9208   74.9667\n",
       "    4        0.0401136                   0.811183           1.70827   1.73955            0.88785          0.90411                     0.0167401       0.0697797                  70.8271   73.9555\n",
       "    5        0.0500046                   0.801505           1.60338   1.71262            0.833333         0.89011                     0.015859        0.0856388                  60.3377   71.2618\n",
       "    6        0.100101                    0.74911            1.5301    1.62127            0.795247         0.842635                    0.076652        0.162291                   53.0097   62.1274\n",
       "    7        0.150105                    0.713825           1.46595   1.56953            0.761905         0.815741                    0.073304        0.235595                   46.5945   56.9529\n",
       "    8        0.20011                     0.680933           1.38842   1.52427            0.721612         0.79222                     0.0694273       0.305022                   38.8419   52.4273\n",
       "    9        0.300027                    0.635633           1.26095   1.43658            0.655362         0.746642                    0.125991        0.431013                   26.0951   43.6579\n",
       "    10       0.400037                    0.575109           1.16817   1.36948            0.607143         0.711767                    0.116828        0.547841                   16.8175   36.9478\n",
       "    11       0.504259                    0.521001           1.08376   1.31042            0.563269         0.681075                    0.112952        0.660793                   8.37591   31.0425\n",
       "    12       0.60033                     0.472511           0.933597  1.25012            0.485224         0.649733                    0.0896916       0.750485                   -6.64033  25.0121\n",
       "    13       0.701072                    0.427509           0.794109  1.18459            0.412727         0.615676                    0.08            0.830485                   -20.5891  18.4593\n",
       "    14       0.800348                    0.392906           0.669159  1.12066            0.347786         0.582447                    0.0664317       0.896916                   -33.0841  12.0658\n",
       "    15       0.900174                    0.36147            0.584277  1.06118            0.30367          0.551531                    0.058326        0.955242                   -41.5723  6.11752\n",
       "    16       1                           0.231394           0.448357  1                  0.233028         0.519736                    0.0447577       1                          -55.1643  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Distributed Random Forest Performance\n",
    "data_rf2.model_performance(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ModelMetricsBinomial: gbm\n",
      "** Reported on test data. **\n",
      "\n",
      "MSE: 0.20994431051192827\n",
      "R^2: 0.1589122788458105\n",
      "LogLoss: 0.6074948423093912\n",
      "AUC: 0.7296870936198954\n",
      "Gini: 0.45937418723979073\n",
      "\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.37325809173716235: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>NO</b></td>\n",
       "<td><b>YES</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>NO</td>\n",
       "<td>2249.0</td>\n",
       "<td>2995.0</td>\n",
       "<td>0.5711</td>\n",
       "<td> (2995.0/5244.0)</td></tr>\n",
       "<tr><td>YES</td>\n",
       "<td>839.0</td>\n",
       "<td>4836.0</td>\n",
       "<td>0.1478</td>\n",
       "<td> (839.0/5675.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>3088.0</td>\n",
       "<td>7831.0</td>\n",
       "<td>0.3511</td>\n",
       "<td> (3834.0/10919.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       NO    YES    Error    Rate\n",
       "-----  ----  -----  -------  ----------------\n",
       "NO     2249  2995   0.5711   (2995.0/5244.0)\n",
       "YES    839   4836   0.1478   (839.0/5675.0)\n",
       "Total  3088  7831   0.3511   (3834.0/10919.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.3732581</td>\n",
       "<td>0.7161262</td>\n",
       "<td>285.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.1949921</td>\n",
       "<td>0.8460707</td>\n",
       "<td>369.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.5645440</td>\n",
       "<td>0.6866944</td>\n",
       "<td>177.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.4746806</td>\n",
       "<td>0.6727722</td>\n",
       "<td>226.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.9572355</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0846539</td>\n",
       "<td>1.0</td>\n",
       "<td>398.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.9572355</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_MCC</td>\n",
       "<td>0.4746806</td>\n",
       "<td>0.3435593</td>\n",
       "<td>226.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.5078917</td>\n",
       "<td>0.6697178</td>\n",
       "<td>209.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                      threshold    value     idx\n",
       "--------------------------  -----------  --------  -----\n",
       "max f1                      0.373258     0.716126  285\n",
       "max f2                      0.194992     0.846071  369\n",
       "max f0point5                0.564544     0.686694  177\n",
       "max accuracy                0.474681     0.672772  226\n",
       "max precision               0.957236     1         0\n",
       "max recall                  0.0846539    1         398\n",
       "max specificity             0.957236     1         0\n",
       "max absolute_MCC            0.474681     0.343559  226\n",
       "max min_per_class_accuracy  0.507892     0.669718  209"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gains/Lift Table: Avg response rate: 51.97 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0100742</td>\n",
       "<td>0.8994217</td>\n",
       "<td>1.8191045</td>\n",
       "<td>1.8191045</td>\n",
       "<td>0.9454545</td>\n",
       "<td>0.9454545</td>\n",
       "<td>0.0183260</td>\n",
       "<td>0.0183260</td>\n",
       "<td>81.9104525</td>\n",
       "<td>81.9104525</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0200568</td>\n",
       "<td>0.8855835</td>\n",
       "<td>1.6769268</td>\n",
       "<td>1.7483403</td>\n",
       "<td>0.8715596</td>\n",
       "<td>0.9086758</td>\n",
       "<td>0.0167401</td>\n",
       "<td>0.0350661</td>\n",
       "<td>67.6926808</td>\n",
       "<td>74.8340273</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0300394</td>\n",
       "<td>0.8727916</td>\n",
       "<td>1.7475343</td>\n",
       "<td>1.7480724</td>\n",
       "<td>0.9082569</td>\n",
       "<td>0.9085366</td>\n",
       "<td>0.0174449</td>\n",
       "<td>0.0525110</td>\n",
       "<td>74.7534252</td>\n",
       "<td>74.8072419</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0401136</td>\n",
       "<td>0.8609245</td>\n",
       "<td>1.7316476</td>\n",
       "<td>1.7439475</td>\n",
       "<td>0.9</td>\n",
       "<td>0.9063927</td>\n",
       "<td>0.0174449</td>\n",
       "<td>0.0699559</td>\n",
       "<td>73.1647577</td>\n",
       "<td>74.3947458</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0500046</td>\n",
       "<td>0.8495728</td>\n",
       "<td>1.6568233</td>\n",
       "<td>1.7267141</td>\n",
       "<td>0.8611111</td>\n",
       "<td>0.8974359</td>\n",
       "<td>0.0163877</td>\n",
       "<td>0.0863436</td>\n",
       "<td>65.6823299</td>\n",
       "<td>72.6714108</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1000092</td>\n",
       "<td>0.7923066</td>\n",
       "<td>1.5082319</td>\n",
       "<td>1.6174730</td>\n",
       "<td>0.7838828</td>\n",
       "<td>0.8406593</td>\n",
       "<td>0.0754185</td>\n",
       "<td>0.1617621</td>\n",
       "<td>50.8231915</td>\n",
       "<td>61.7473012</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1504717</td>\n",
       "<td>0.7499476</td>\n",
       "<td>1.4456586</td>\n",
       "<td>1.5598530</td>\n",
       "<td>0.7513612</td>\n",
       "<td>0.8107121</td>\n",
       "<td>0.0729515</td>\n",
       "<td>0.2347137</td>\n",
       "<td>44.5658594</td>\n",
       "<td>55.9852960</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2001099</td>\n",
       "<td>0.7143123</td>\n",
       "<td>1.4057656</td>\n",
       "<td>1.5216308</td>\n",
       "<td>0.7306273</td>\n",
       "<td>0.7908467</td>\n",
       "<td>0.0697797</td>\n",
       "<td>0.3044934</td>\n",
       "<td>40.5765561</td>\n",
       "<td>52.1630823</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3001191</td>\n",
       "<td>0.6513619</td>\n",
       "<td>1.3478942</td>\n",
       "<td>1.4637363</td>\n",
       "<td>0.7005495</td>\n",
       "<td>0.7607568</td>\n",
       "<td>0.1348018</td>\n",
       "<td>0.4392952</td>\n",
       "<td>34.7894176</td>\n",
       "<td>46.3736280</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4001282</td>\n",
       "<td>0.5829166</td>\n",
       "<td>1.2016521</td>\n",
       "<td>1.3982302</td>\n",
       "<td>0.6245421</td>\n",
       "<td>0.7267109</td>\n",
       "<td>0.1201762</td>\n",
       "<td>0.5594714</td>\n",
       "<td>20.1652063</td>\n",
       "<td>39.8230222</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5001374</td>\n",
       "<td>0.5111190</td>\n",
       "<td>1.0342665</td>\n",
       "<td>1.3254508</td>\n",
       "<td>0.5375458</td>\n",
       "<td>0.6888848</td>\n",
       "<td>0.1034361</td>\n",
       "<td>0.6629075</td>\n",
       "<td>3.4266512</td>\n",
       "<td>32.5450810</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.5999634</td>\n",
       "<td>0.4450936</td>\n",
       "<td>0.9602612</td>\n",
       "<td>1.2646881</td>\n",
       "<td>0.4990826</td>\n",
       "<td>0.6573042</td>\n",
       "<td>0.0958590</td>\n",
       "<td>0.7587665</td>\n",
       "<td>-3.9738754</td>\n",
       "<td>26.4688083</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.6999725</td>\n",
       "<td>0.3819586</td>\n",
       "<td>0.8016887</td>\n",
       "<td>1.1985367</td>\n",
       "<td>0.4166667</td>\n",
       "<td>0.6229229</td>\n",
       "<td>0.0801762</td>\n",
       "<td>0.8389427</td>\n",
       "<td>-19.8311307</td>\n",
       "<td>19.8536659</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.7999817</td>\n",
       "<td>0.3246684</td>\n",
       "<td>0.6448749</td>\n",
       "<td>1.1293210</td>\n",
       "<td>0.3351648</td>\n",
       "<td>0.5869491</td>\n",
       "<td>0.0644934</td>\n",
       "<td>0.9034361</td>\n",
       "<td>-35.5125139</td>\n",
       "<td>12.9321011</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.8999908</td>\n",
       "<td>0.2601986</td>\n",
       "<td>0.5497294</td>\n",
       "<td>1.0649154</td>\n",
       "<td>0.2857143</td>\n",
       "<td>0.5534751</td>\n",
       "<td>0.0549780</td>\n",
       "<td>0.9584141</td>\n",
       "<td>-45.0270610</td>\n",
       "<td>6.4915389</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0799673</td>\n",
       "<td>0.4158209</td>\n",
       "<td>1.0</td>\n",
       "<td>0.2161172</td>\n",
       "<td>0.5197362</td>\n",
       "<td>0.0415859</td>\n",
       "<td>1.0</td>\n",
       "<td>-58.4179051</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift      cumulative_lift    response_rate    cumulative_response_rate    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  --------  -----------------  ---------------  --------------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.0100742                   0.899422           1.8191    1.8191             0.945455         0.945455                    0.018326        0.018326                   81.9105   81.9105\n",
       "    2        0.0200568                   0.885584           1.67693   1.74834            0.87156          0.908676                    0.0167401       0.0350661                  67.6927   74.834\n",
       "    3        0.0300394                   0.872792           1.74753   1.74807            0.908257         0.908537                    0.0174449       0.052511                   74.7534   74.8072\n",
       "    4        0.0401136                   0.860924           1.73165   1.74395            0.9              0.906393                    0.0174449       0.0699559                  73.1648   74.3947\n",
       "    5        0.0500046                   0.849573           1.65682   1.72671            0.861111         0.897436                    0.0163877       0.0863436                  65.6823   72.6714\n",
       "    6        0.100009                    0.792307           1.50823   1.61747            0.783883         0.840659                    0.0754185       0.161762                   50.8232   61.7473\n",
       "    7        0.150472                    0.749948           1.44566   1.55985            0.751361         0.810712                    0.0729515       0.234714                   44.5659   55.9853\n",
       "    8        0.20011                     0.714312           1.40577   1.52163            0.730627         0.790847                    0.0697797       0.304493                   40.5766   52.1631\n",
       "    9        0.300119                    0.651362           1.34789   1.46374            0.700549         0.760757                    0.134802        0.439295                   34.7894   46.3736\n",
       "    10       0.400128                    0.582917           1.20165   1.39823            0.624542         0.726711                    0.120176        0.559471                   20.1652   39.823\n",
       "    11       0.500137                    0.511119           1.03427   1.32545            0.537546         0.688885                    0.103436        0.662907                   3.42665   32.5451\n",
       "    12       0.599963                    0.445094           0.960261  1.26469            0.499083         0.657304                    0.095859        0.758767                   -3.97388  26.4688\n",
       "    13       0.699973                    0.381959           0.801689  1.19854            0.416667         0.622923                    0.0801762       0.838943                   -19.8311  19.8537\n",
       "    14       0.799982                    0.324668           0.644875  1.12932            0.335165         0.586949                    0.0644934       0.903436                   -35.5125  12.9321\n",
       "    15       0.899991                    0.260199           0.549729  1.06492            0.285714         0.553475                    0.054978        0.958414                   -45.0271  6.49154\n",
       "    16       1                           0.0799673          0.415821  1                  0.216117         0.519736                    0.0415859       1                          -58.4179  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GBM Performance\n",
    "data_gbm2.model_performance(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ModelMetricsBinomial: deeplearning\n",
      "** Reported on test data. **\n",
      "\n",
      "MSE: 0.22280968518794017\n",
      "R^2: 0.10737047406121802\n",
      "LogLoss: 0.6374853419025917\n",
      "AUC: 0.708553916874162\n",
      "Gini: 0.41710783374832405\n",
      "\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.25611323581363243: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>NO</b></td>\n",
       "<td><b>YES</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>NO</td>\n",
       "<td>1714.0</td>\n",
       "<td>3530.0</td>\n",
       "<td>0.6732</td>\n",
       "<td> (3530.0/5244.0)</td></tr>\n",
       "<tr><td>YES</td>\n",
       "<td>627.0</td>\n",
       "<td>5048.0</td>\n",
       "<td>0.1105</td>\n",
       "<td> (627.0/5675.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>2341.0</td>\n",
       "<td>8578.0</td>\n",
       "<td>0.3807</td>\n",
       "<td> (4157.0/10919.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       NO    YES    Error    Rate\n",
       "-----  ----  -----  -------  ----------------\n",
       "NO     1714  3530   0.6732   (3530.0/5244.0)\n",
       "YES    627   5048   0.1105   (627.0/5675.0)\n",
       "Total  2341  8578   0.3807   (4157.0/10919.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.2561132</td>\n",
       "<td>0.7083421</td>\n",
       "<td>315.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.0759424</td>\n",
       "<td>0.8445140</td>\n",
       "<td>391.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.4576310</td>\n",
       "<td>0.6688179</td>\n",
       "<td>214.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.4132508</td>\n",
       "<td>0.6553714</td>\n",
       "<td>237.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.9607276</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0476740</td>\n",
       "<td>1.0</td>\n",
       "<td>398.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.9607276</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_MCC</td>\n",
       "<td>0.4132508</td>\n",
       "<td>0.3090235</td>\n",
       "<td>237.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.4319226</td>\n",
       "<td>0.6519824</td>\n",
       "<td>227.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                      threshold    value     idx\n",
       "--------------------------  -----------  --------  -----\n",
       "max f1                      0.256113     0.708342  315\n",
       "max f2                      0.0759424    0.844514  391\n",
       "max f0point5                0.457631     0.668818  214\n",
       "max accuracy                0.413251     0.655371  237\n",
       "max precision               0.960728     1         0\n",
       "max recall                  0.047674     1         398\n",
       "max specificity             0.960728     1         0\n",
       "max absolute_MCC            0.413251     0.309023  237\n",
       "max min_per_class_accuracy  0.431923     0.651982  227"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gains/Lift Table: Avg response rate: 51.97 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0101658</td>\n",
       "<td>0.9026969</td>\n",
       "<td>1.8373838</td>\n",
       "<td>1.8373838</td>\n",
       "<td>0.9549550</td>\n",
       "<td>0.9549550</td>\n",
       "<td>0.0186784</td>\n",
       "<td>0.0186784</td>\n",
       "<td>83.7383816</td>\n",
       "<td>83.7383816</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0201484</td>\n",
       "<td>0.8678130</td>\n",
       "<td>1.6769268</td>\n",
       "<td>1.7578847</td>\n",
       "<td>0.8715596</td>\n",
       "<td>0.9136364</td>\n",
       "<td>0.0167401</td>\n",
       "<td>0.0354185</td>\n",
       "<td>67.6926808</td>\n",
       "<td>75.7884662</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0300394</td>\n",
       "<td>0.8432173</td>\n",
       "<td>1.6390080</td>\n",
       "<td>1.7187423</td>\n",
       "<td>0.8518519</td>\n",
       "<td>0.8932927</td>\n",
       "<td>0.0162115</td>\n",
       "<td>0.0516300</td>\n",
       "<td>63.9007995</td>\n",
       "<td>71.8742344</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0400220</td>\n",
       "<td>0.8207134</td>\n",
       "<td>1.6945787</td>\n",
       "<td>1.7127152</td>\n",
       "<td>0.8807339</td>\n",
       "<td>0.8901602</td>\n",
       "<td>0.0169163</td>\n",
       "<td>0.0685463</td>\n",
       "<td>69.4578669</td>\n",
       "<td>71.2715249</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0500046</td>\n",
       "<td>0.8009048</td>\n",
       "<td>1.3944970</td>\n",
       "<td>1.6491882</td>\n",
       "<td>0.7247706</td>\n",
       "<td>0.8571429</td>\n",
       "<td>0.0139207</td>\n",
       "<td>0.0824670</td>\n",
       "<td>39.4497029</td>\n",
       "<td>64.9188169</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1000092</td>\n",
       "<td>0.7365274</td>\n",
       "<td>1.4800407</td>\n",
       "<td>1.5646144</td>\n",
       "<td>0.7692308</td>\n",
       "<td>0.8131868</td>\n",
       "<td>0.0740088</td>\n",
       "<td>0.1564758</td>\n",
       "<td>48.0040664</td>\n",
       "<td>56.4614416</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1500137</td>\n",
       "<td>0.6847974</td>\n",
       "<td>1.4307060</td>\n",
       "<td>1.5199783</td>\n",
       "<td>0.7435897</td>\n",
       "<td>0.7899878</td>\n",
       "<td>0.0715419</td>\n",
       "<td>0.2280176</td>\n",
       "<td>43.0705975</td>\n",
       "<td>51.9978269</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2001099</td>\n",
       "<td>0.6468197</td>\n",
       "<td>1.3225665</td>\n",
       "<td>1.4705576</td>\n",
       "<td>0.6873857</td>\n",
       "<td>0.7643021</td>\n",
       "<td>0.0662555</td>\n",
       "<td>0.2942731</td>\n",
       "<td>32.2566502</td>\n",
       "<td>47.0557566</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3000275</td>\n",
       "<td>0.5703766</td>\n",
       "<td>1.3244397</td>\n",
       "<td>1.4218962</td>\n",
       "<td>0.6883593</td>\n",
       "<td>0.7390110</td>\n",
       "<td>0.1323348</td>\n",
       "<td>0.4266079</td>\n",
       "<td>32.4439689</td>\n",
       "<td>42.1896210</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4002198</td>\n",
       "<td>0.5007653</td>\n",
       "<td>1.1414171</td>\n",
       "<td>1.3516802</td>\n",
       "<td>0.5932358</td>\n",
       "<td>0.7025172</td>\n",
       "<td>0.1143612</td>\n",
       "<td>0.5409692</td>\n",
       "<td>14.1417101</td>\n",
       "<td>35.1680158</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5000458</td>\n",
       "<td>0.4324134</td>\n",
       "<td>1.0555813</td>\n",
       "<td>1.2925688</td>\n",
       "<td>0.5486239</td>\n",
       "<td>0.6717949</td>\n",
       "<td>0.1053744</td>\n",
       "<td>0.6463436</td>\n",
       "<td>5.5581296</td>\n",
       "<td>29.2568847</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.5999634</td>\n",
       "<td>0.3722628</td>\n",
       "<td>0.9664354</td>\n",
       "<td>1.2382548</td>\n",
       "<td>0.5022915</td>\n",
       "<td>0.6435659</td>\n",
       "<td>0.0965639</td>\n",
       "<td>0.7429075</td>\n",
       "<td>-3.3564648</td>\n",
       "<td>23.8254751</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.6999725</td>\n",
       "<td>0.3106081</td>\n",
       "<td>0.8439756</td>\n",
       "<td>1.1819218</td>\n",
       "<td>0.4386447</td>\n",
       "<td>0.6142876</td>\n",
       "<td>0.0844053</td>\n",
       "<td>0.8273128</td>\n",
       "<td>-15.6024431</td>\n",
       "<td>18.1921784</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.8000733</td>\n",
       "<td>0.2454952</td>\n",
       "<td>0.6900537</td>\n",
       "<td>1.1203820</td>\n",
       "<td>0.3586459</td>\n",
       "<td>0.5823031</td>\n",
       "<td>0.0690749</td>\n",
       "<td>0.8963877</td>\n",
       "<td>-30.9946274</td>\n",
       "<td>12.0381973</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.9000824</td>\n",
       "<td>0.1787110</td>\n",
       "<td>0.6008260</td>\n",
       "<td>1.0626535</td>\n",
       "<td>0.3122711</td>\n",
       "<td>0.5522996</td>\n",
       "<td>0.0600881</td>\n",
       "<td>0.9564758</td>\n",
       "<td>-39.9173968</td>\n",
       "<td>6.2653535</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0395820</td>\n",
       "<td>0.4356013</td>\n",
       "<td>1.0</td>\n",
       "<td>0.2263978</td>\n",
       "<td>0.5197362</td>\n",
       "<td>0.0435242</td>\n",
       "<td>1.0</td>\n",
       "<td>-56.4398664</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift      cumulative_lift    response_rate    cumulative_response_rate    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  --------  -----------------  ---------------  --------------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.0101658                   0.902697           1.83738   1.83738            0.954955         0.954955                    0.0186784       0.0186784                  83.7384   83.7384\n",
       "    2        0.0201484                   0.867813           1.67693   1.75788            0.87156          0.913636                    0.0167401       0.0354185                  67.6927   75.7885\n",
       "    3        0.0300394                   0.843217           1.63901   1.71874            0.851852         0.893293                    0.0162115       0.05163                    63.9008   71.8742\n",
       "    4        0.040022                    0.820713           1.69458   1.71272            0.880734         0.89016                     0.0169163       0.0685463                  69.4579   71.2715\n",
       "    5        0.0500046                   0.800905           1.3945    1.64919            0.724771         0.857143                    0.0139207       0.082467                   39.4497   64.9188\n",
       "    6        0.100009                    0.736527           1.48004   1.56461            0.769231         0.813187                    0.0740088       0.156476                   48.0041   56.4614\n",
       "    7        0.150014                    0.684797           1.43071   1.51998            0.74359          0.789988                    0.0715419       0.228018                   43.0706   51.9978\n",
       "    8        0.20011                     0.64682            1.32257   1.47056            0.687386         0.764302                    0.0662555       0.294273                   32.2567   47.0558\n",
       "    9        0.300027                    0.570377           1.32444   1.4219             0.688359         0.739011                    0.132335        0.426608                   32.444    42.1896\n",
       "    10       0.40022                     0.500765           1.14142   1.35168            0.593236         0.702517                    0.114361        0.540969                   14.1417   35.168\n",
       "    11       0.500046                    0.432413           1.05558   1.29257            0.548624         0.671795                    0.105374        0.646344                   5.55813   29.2569\n",
       "    12       0.599963                    0.372263           0.966435  1.23825            0.502291         0.643566                    0.0965639       0.742907                   -3.35646  23.8255\n",
       "    13       0.699973                    0.310608           0.843976  1.18192            0.438645         0.614288                    0.0844053       0.827313                   -15.6024  18.1922\n",
       "    14       0.800073                    0.245495           0.690054  1.12038            0.358646         0.582303                    0.0690749       0.896388                   -30.9946  12.0382\n",
       "    15       0.900082                    0.178711           0.600826  1.06265            0.312271         0.5523                      0.0600881       0.956476                   -39.9174  6.26535\n",
       "    16       1                           0.039582           0.435601  1                  0.226398         0.519736                    0.0435242       1                          -56.4399  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Deep Learning Performance\n",
    "data_dl.model_performance(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ModelMetricsBinomial: naivebayes\n",
      "** Reported on test data. **\n",
      "\n",
      "MSE: 0.23152996070216778\n",
      "R^2: 0.07243493976541415\n",
      "LogLoss: 0.6647084512439999\n",
      "AUC: 0.6877895778519272\n",
      "Gini: 0.37557915570385436\n",
      "\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.22770709644158957: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>NO</b></td>\n",
       "<td><b>YES</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>NO</td>\n",
       "<td>1199.0</td>\n",
       "<td>4045.0</td>\n",
       "<td>0.7714</td>\n",
       "<td> (4045.0/5244.0)</td></tr>\n",
       "<tr><td>YES</td>\n",
       "<td>425.0</td>\n",
       "<td>5250.0</td>\n",
       "<td>0.0749</td>\n",
       "<td> (425.0/5675.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>1624.0</td>\n",
       "<td>9295.0</td>\n",
       "<td>0.4094</td>\n",
       "<td> (4470.0/10919.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       NO    YES    Error    Rate\n",
       "-----  ----  -----  -------  ----------------\n",
       "NO     1199  4045   0.7714   (4045.0/5244.0)\n",
       "YES    425   5250   0.0749   (425.0/5675.0)\n",
       "Total  1624  9295   0.4094   (4470.0/10919.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.2277071</td>\n",
       "<td>0.7014028</td>\n",
       "<td>334.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.0639758</td>\n",
       "<td>0.8453940</td>\n",
       "<td>390.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.5630666</td>\n",
       "<td>0.6520698</td>\n",
       "<td>188.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.4959973</td>\n",
       "<td>0.6370547</td>\n",
       "<td>218.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.9974487</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0207270</td>\n",
       "<td>1.0</td>\n",
       "<td>398.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.9974487</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_MCC</td>\n",
       "<td>0.5630666</td>\n",
       "<td>0.2765067</td>\n",
       "<td>188.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.4985902</td>\n",
       "<td>0.6366520</td>\n",
       "<td>217.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                      threshold    value     idx\n",
       "--------------------------  -----------  --------  -----\n",
       "max f1                      0.227707     0.701403  334\n",
       "max f2                      0.0639758    0.845394  390\n",
       "max f0point5                0.563067     0.65207   188\n",
       "max accuracy                0.495997     0.637055  218\n",
       "max precision               0.997449     1         0\n",
       "max recall                  0.020727     1         398\n",
       "max specificity             0.997449     1         0\n",
       "max absolute_MCC            0.563067     0.276507  188\n",
       "max min_per_class_accuracy  0.49859      0.636652  217"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gains/Lift Table: Avg response rate: 51.97 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0100742</td>\n",
       "<td>0.9849015</td>\n",
       "<td>1.8365959</td>\n",
       "<td>1.8365959</td>\n",
       "<td>0.9545455</td>\n",
       "<td>0.9545455</td>\n",
       "<td>0.0185022</td>\n",
       "<td>0.0185022</td>\n",
       "<td>83.6595915</td>\n",
       "<td>83.6595915</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0200568</td>\n",
       "<td>0.9736685</td>\n",
       "<td>1.6945787</td>\n",
       "<td>1.7659115</td>\n",
       "<td>0.8807339</td>\n",
       "<td>0.9178082</td>\n",
       "<td>0.0169163</td>\n",
       "<td>0.0354185</td>\n",
       "<td>69.4578669</td>\n",
       "<td>76.5911532</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0300394</td>\n",
       "<td>0.9599325</td>\n",
       "<td>1.5533638</td>\n",
       "<td>1.6952783</td>\n",
       "<td>0.8073394</td>\n",
       "<td>0.8810976</td>\n",
       "<td>0.0155066</td>\n",
       "<td>0.0509251</td>\n",
       "<td>55.3363780</td>\n",
       "<td>69.5278285</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0400220</td>\n",
       "<td>0.9461100</td>\n",
       "<td>1.4651045</td>\n",
       "<td>1.6378665</td>\n",
       "<td>0.7614679</td>\n",
       "<td>0.8512586</td>\n",
       "<td>0.0146256</td>\n",
       "<td>0.0655507</td>\n",
       "<td>46.5104474</td>\n",
       "<td>63.7866511</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0500046</td>\n",
       "<td>0.9340485</td>\n",
       "<td>1.5533638</td>\n",
       "<td>1.6209969</td>\n",
       "<td>0.8073394</td>\n",
       "<td>0.8424908</td>\n",
       "<td>0.0155066</td>\n",
       "<td>0.0810573</td>\n",
       "<td>55.3363780</td>\n",
       "<td>62.0996918</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1000092</td>\n",
       "<td>0.8714906</td>\n",
       "<td>1.5328993</td>\n",
       "<td>1.5769481</td>\n",
       "<td>0.7967033</td>\n",
       "<td>0.8195971</td>\n",
       "<td>0.0766520</td>\n",
       "<td>0.1577093</td>\n",
       "<td>53.2899259</td>\n",
       "<td>57.6948089</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1501053</td>\n",
       "<td>0.8159428</td>\n",
       "<td>1.3682935</td>\n",
       "<td>1.5073117</td>\n",
       "<td>0.7111517</td>\n",
       "<td>0.7834045</td>\n",
       "<td>0.0685463</td>\n",
       "<td>0.2262555</td>\n",
       "<td>36.8293535</td>\n",
       "<td>50.7311700</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2001099</td>\n",
       "<td>0.7705500</td>\n",
       "<td>1.3743235</td>\n",
       "<td>1.4740799</td>\n",
       "<td>0.7142857</td>\n",
       "<td>0.7661327</td>\n",
       "<td>0.0687225</td>\n",
       "<td>0.2949780</td>\n",
       "<td>37.4323474</td>\n",
       "<td>47.4079860</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3001191</td>\n",
       "<td>0.6889571</td>\n",
       "<td>1.2227955</td>\n",
       "<td>1.3903440</td>\n",
       "<td>0.6355311</td>\n",
       "<td>0.7226121</td>\n",
       "<td>0.1222907</td>\n",
       "<td>0.4172687</td>\n",
       "<td>22.2795501</td>\n",
       "<td>39.0343967</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4001282</td>\n",
       "<td>0.6054809</td>\n",
       "<td>1.0941729</td>\n",
       "<td>1.3163182</td>\n",
       "<td>0.5686813</td>\n",
       "<td>0.6841382</td>\n",
       "<td>0.1094273</td>\n",
       "<td>0.5266960</td>\n",
       "<td>9.4172920</td>\n",
       "<td>31.6318153</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5001374</td>\n",
       "<td>0.5012963</td>\n",
       "<td>1.0413143</td>\n",
       "<td>1.2613275</td>\n",
       "<td>0.5412088</td>\n",
       "<td>0.6555576</td>\n",
       "<td>0.1041410</td>\n",
       "<td>0.6308370</td>\n",
       "<td>4.1314324</td>\n",
       "<td>26.1327459</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.5999634</td>\n",
       "<td>0.4084338</td>\n",
       "<td>0.8772975</td>\n",
       "<td>1.1974299</td>\n",
       "<td>0.4559633</td>\n",
       "<td>0.6223477</td>\n",
       "<td>0.0875771</td>\n",
       "<td>0.7184141</td>\n",
       "<td>-12.2702502</td>\n",
       "<td>19.7429938</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.6999725</td>\n",
       "<td>0.3306943</td>\n",
       "<td>0.8545473</td>\n",
       "<td>1.1484403</td>\n",
       "<td>0.4441392</td>\n",
       "<td>0.5968860</td>\n",
       "<td>0.0854626</td>\n",
       "<td>0.8038767</td>\n",
       "<td>-14.5452712</td>\n",
       "<td>14.8440293</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.7999817</td>\n",
       "<td>0.2637767</td>\n",
       "<td>0.8175463</td>\n",
       "<td>1.1070738</td>\n",
       "<td>0.4249084</td>\n",
       "<td>0.5753864</td>\n",
       "<td>0.0817621</td>\n",
       "<td>0.8856388</td>\n",
       "<td>-18.2453728</td>\n",
       "<td>10.7073806</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.8999908</td>\n",
       "<td>0.1810967</td>\n",
       "<td>0.6624944</td>\n",
       "<td>1.0576711</td>\n",
       "<td>0.3443223</td>\n",
       "<td>0.5497100</td>\n",
       "<td>0.0662555</td>\n",
       "<td>0.9518943</td>\n",
       "<td>-33.7505607</td>\n",
       "<td>5.7671066</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0126029</td>\n",
       "<td>0.4810132</td>\n",
       "<td>1.0</td>\n",
       "<td>0.25</td>\n",
       "<td>0.5197362</td>\n",
       "<td>0.0481057</td>\n",
       "<td>1.0</td>\n",
       "<td>-51.8986784</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift      cumulative_lift    response_rate    cumulative_response_rate    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  --------  -----------------  ---------------  --------------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.0100742                   0.984901           1.8366    1.8366             0.954545         0.954545                    0.0185022       0.0185022                  83.6596   83.6596\n",
       "    2        0.0200568                   0.973668           1.69458   1.76591            0.880734         0.917808                    0.0169163       0.0354185                  69.4579   76.5912\n",
       "    3        0.0300394                   0.959933           1.55336   1.69528            0.807339         0.881098                    0.0155066       0.0509251                  55.3364   69.5278\n",
       "    4        0.040022                    0.94611            1.4651    1.63787            0.761468         0.851259                    0.0146256       0.0655507                  46.5104   63.7867\n",
       "    5        0.0500046                   0.934049           1.55336   1.621              0.807339         0.842491                    0.0155066       0.0810573                  55.3364   62.0997\n",
       "    6        0.100009                    0.871491           1.5329    1.57695            0.796703         0.819597                    0.076652        0.157709                   53.2899   57.6948\n",
       "    7        0.150105                    0.815943           1.36829   1.50731            0.711152         0.783405                    0.0685463       0.226256                   36.8294   50.7312\n",
       "    8        0.20011                     0.77055            1.37432   1.47408            0.714286         0.766133                    0.0687225       0.294978                   37.4323   47.408\n",
       "    9        0.300119                    0.688957           1.2228    1.39034            0.635531         0.722612                    0.122291        0.417269                   22.2796   39.0344\n",
       "    10       0.400128                    0.605481           1.09417   1.31632            0.568681         0.684138                    0.109427        0.526696                   9.41729   31.6318\n",
       "    11       0.500137                    0.501296           1.04131   1.26133            0.541209         0.655558                    0.104141        0.630837                   4.13143   26.1327\n",
       "    12       0.599963                    0.408434           0.877297  1.19743            0.455963         0.622348                    0.0875771       0.718414                   -12.2703  19.743\n",
       "    13       0.699973                    0.330694           0.854547  1.14844            0.444139         0.596886                    0.0854626       0.803877                   -14.5453  14.844\n",
       "    14       0.799982                    0.263777           0.817546  1.10707            0.424908         0.575386                    0.0817621       0.885639                   -18.2454  10.7074\n",
       "    15       0.899991                    0.181097           0.662494  1.05767            0.344322         0.54971                     0.0662555       0.951894                   -33.7506  5.76711\n",
       "    16       1                           0.0126029          0.481013  1                  0.25             0.519736                    0.0481057       1                          -51.8987  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive Bayes Performance\n",
    "data_nb.model_performance(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GLM Coefficient Magnitudes / DRF, GBM & Deep Learning Variable Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient Magnitudes:\n",
      "\n",
      "| Predictor                 |   Normalized Coefficient |\n",
      "|---------------------------+--------------------------|\n",
      "| Year.2008                 |              2.30549     |\n",
      "| Origin.HPN                |              1.711       |\n",
      "| Origin.LIH                |              1.60953     |\n",
      "| Origin.MDW                |              1.56914     |\n",
      "| Year.2007                 |              1.52842     |\n",
      "| Origin.CHO                |              1.4856      |\n",
      "| Year.2003                 |              1.47982     |\n",
      "| Dest.LYH                  |              1.3856      |\n",
      "| Dest.HTS                  |              1.22876     |\n",
      "| Origin.PSP                |              1.14947     |\n",
      "| Origin.ATL                |              1.14637     |\n",
      "| Origin.TLH                |              1.13651     |\n",
      "| UniqueCarrier.HP          |              1.11993     |\n",
      "| Origin.LEX                |              1.07531     |\n",
      "| UniqueCarrier.TW          |              1.02583     |\n",
      "| Year.2001                 |              1.0066      |\n",
      "| Origin.TRI                |              1.0045      |\n",
      "| Origin.HNL                |              0.986245    |\n",
      "| Year.2002                 |              0.984703    |\n",
      "| Origin.LBB                |              0.956915    |\n",
      "| Origin.PBI                |              0.862223    |\n",
      "| Origin.CAE                |              0.841282    |\n",
      "| Origin.OGG                |              0.83888     |\n",
      "| Origin.EGE                |              0.83154     |\n",
      "| Origin.MYR                |              0.818385    |\n",
      "| Origin.MLB                |              0.810896    |\n",
      "| Year.2006                 |              0.802285    |\n",
      "| Origin.BTV                |              0.796766    |\n",
      "| Origin.IAH                |              0.79206     |\n",
      "| Year.2004                 |              0.782       |\n",
      "| Year.1994                 |              0.77487     |\n",
      "| Origin.CRP                |              0.757592    |\n",
      "| Dest.ICT                  |              0.746594    |\n",
      "| Origin.ORD                |              0.730829    |\n",
      "| Dest.CAE                  |              0.722723    |\n",
      "| Origin.OMA                |              0.714599    |\n",
      "| Origin.SDF                |              0.702793    |\n",
      "| Origin.ERI                |              0.700394    |\n",
      "| Origin.STL                |              0.68777     |\n",
      "| Dest.PBI                  |              0.678256    |\n",
      "| Dest.CHO                  |              0.673908    |\n",
      "| Origin.LYH                |              0.673908    |\n",
      "| Dest.FLL                  |              0.660198    |\n",
      "| Dest.ISP                  |              0.645513    |\n",
      "| Year.1996                 |              0.645047    |\n",
      "| Origin.CMH                |              0.643075    |\n",
      "| Origin.MRY                |              0.641558    |\n",
      "| Origin.PIT                |              0.626292    |\n",
      "| Origin.FLL                |              0.626133    |\n",
      "| Dest.PSP                  |              0.622363    |\n",
      "| Dest.JAX                  |              0.611633    |\n",
      "| Origin.OKC                |              0.610629    |\n",
      "| Origin.GSO                |              0.596394    |\n",
      "| Origin.SRQ                |              0.587497    |\n",
      "| Origin.MIA                |              0.584298    |\n",
      "| Dest.CHS                  |              0.583466    |\n",
      "| Origin.ACY                |              0.582019    |\n",
      "| Year.1997                 |              0.574399    |\n",
      "| Origin.SAV                |              0.573883    |\n",
      "| Year.1990                 |              0.571829    |\n",
      "| Dest.IAH                  |              0.561823    |\n",
      "| Origin.AUS                |              0.551613    |\n",
      "| Origin.BDL                |              0.532285    |\n",
      "| Dest.FAY                  |              0.530787    |\n",
      "| UniqueCarrier.CO          |              0.525104    |\n",
      "| Origin.MSY                |              0.524022    |\n",
      "| Dest.DAY                  |              0.519766    |\n",
      "| Origin.PWM                |              0.519567    |\n",
      "| Origin.EYW                |              0.516919    |\n",
      "| Dest.SWF                  |              0.516161    |\n",
      "| Origin.LAX                |              0.514366    |\n",
      "| Origin.TYS                |              0.512968    |\n",
      "| Dest.TPA                  |              0.511093    |\n",
      "| Origin.JAX                |              0.50522     |\n",
      "| Dest.ABQ                  |              0.498411    |\n",
      "| Origin.ALB                |              0.496553    |\n",
      "| Dest.GSO                  |              0.494375    |\n",
      "| UniqueCarrier.WN          |              0.494168    |\n",
      "| Dest.IND                  |              0.48799     |\n",
      "| Origin.CRW                |              0.482652    |\n",
      "| UniqueCarrier.PI          |              0.47286     |\n",
      "| Dest.LIH                  |              0.471824    |\n",
      "| Origin.SYR                |              0.469168    |\n",
      "| Origin.PHL                |              0.464968    |\n",
      "| Origin.STX                |              0.463865    |\n",
      "| Origin.ROA                |              0.456348    |\n",
      "| Dest.FAT                  |              0.455259    |\n",
      "| Dest.MKE                  |              0.450209    |\n",
      "| Origin.MCO                |              0.447501    |\n",
      "| Origin.TUS                |              0.438616    |\n",
      "| Dest.BDL                  |              0.43497     |\n",
      "| Dest.SFO                  |              0.429193    |\n",
      "| Dest.KOA                  |              0.428773    |\n",
      "| Origin.LGA                |              0.428413    |\n",
      "| Dest.UCA                  |              0.425415    |\n",
      "| Origin.IND                |              0.422298    |\n",
      "| Year.2005                 |              0.395417    |\n",
      "| Dest.OMA                  |              0.385982    |\n",
      "| Origin.DFW                |              0.382929    |\n",
      "| Origin.PHF                |              0.376555    |\n",
      "| Origin.MCI                |              0.374189    |\n",
      "| Origin.BOI                |              0.374042    |\n",
      "| UniqueCarrier.US          |              0.357643    |\n",
      "| Dest.CLE                  |              0.347673    |\n",
      "| Dest.BUR                  |              0.338592    |\n",
      "| Dest.OAJ                  |              0.327469    |\n",
      "| Dest.PNS                  |              0.325638    |\n",
      "| Origin.BWI                |              0.320807    |\n",
      "| Origin.DAY                |              0.316897    |\n",
      "| Origin.ROC                |              0.315691    |\n",
      "| Dest.MCO                  |              0.314145    |\n",
      "| Dest.GEG                  |              0.313127    |\n",
      "| Year.1992                 |              0.30405     |\n",
      "| Origin.IAD                |              0.301453    |\n",
      "| Dest.STL                  |              0.298754    |\n",
      "| Origin.OAK                |              0.294989    |\n",
      "| Origin.BOS                |              0.293293    |\n",
      "| Dest.LBB                  |              0.285082    |\n",
      "| Origin.RNO                |              0.285007    |\n",
      "| Origin.TPA                |              0.284779    |\n",
      "| Dest.HRL                  |              0.281081    |\n",
      "| Year.1995                 |              0.280706    |\n",
      "| Origin.CLT                |              0.280685    |\n",
      "| Dest.ALB                  |              0.280451    |\n",
      "| Dest.CMH                  |              0.275381    |\n",
      "| Dest.SLC                  |              0.273277    |\n",
      "| Dest.COS                  |              0.269765    |\n",
      "| Dest.SEA                  |              0.268437    |\n",
      "| Year.1987                 |              0.267545    |\n",
      "| Month.10                  |              0.267545    |\n",
      "| Origin.SAN                |              0.266727    |\n",
      "| DayOfWeek.5               |              0.266206    |\n",
      "| Year.1991                 |              0.266096    |\n",
      "| Dest.SBN                  |              0.262734    |\n",
      "| Dest.AVL                  |              0.261969    |\n",
      "| Dest.MHT                  |              0.261497    |\n",
      "| Origin.UCA                |              0.258631    |\n",
      "| Dest.BUF                  |              0.253053    |\n",
      "| Origin.DEN                |              0.235533    |\n",
      "| Origin.ISP                |              0.234582    |\n",
      "| Dest.ROC                  |              0.234288    |\n",
      "| Origin.MKE                |              0.233137    |\n",
      "| Origin.BUF                |              0.227863    |\n",
      "| Dest.SMF                  |              0.22781     |\n",
      "| Dest.PDX                  |              0.225074    |\n",
      "| Origin.BUR                |              0.222947    |\n",
      "| Origin.BHM                |              0.22286     |\n",
      "| Origin.ORF                |              0.221614    |\n",
      "| Dest.ONT                  |              0.215523    |\n",
      "| Dest.SAT                  |              0.20575     |\n",
      "| Origin.PVD                |              0.202666    |\n",
      "| Dest.LAS                  |              0.200266    |\n",
      "| Origin.CVG                |              0.199784    |\n",
      "| Year.1999                 |              0.197594    |\n",
      "| Dest.CLT                  |              0.191104    |\n",
      "| Dest.LAX                  |              0.186343    |\n",
      "| Origin.CLE                |              0.184635    |\n",
      "| Year.2000                 |              0.183435    |\n",
      "| Dest.AUS                  |              0.175129    |\n",
      "| Dest.BTV                  |              0.172214    |\n",
      "| Dest.BNA                  |              0.170666    |\n",
      "| Dest.PVD                  |              0.168254    |\n",
      "| Dest.SNA                  |              0.166623    |\n",
      "| UniqueCarrier.AA          |              0.165028    |\n",
      "| Dest.JFK                  |              0.164523    |\n",
      "| Dest.MCI                  |              0.162499    |\n",
      "| Origin.SLC                |              0.161674    |\n",
      "| Origin.SEA                |              0.158698    |\n",
      "| Year.1993                 |              0.157723    |\n",
      "| Distance                  |              0.15498     |\n",
      "| Origin.ELP                |              0.154511    |\n",
      "| DayOfWeek.2               |              0.154245    |\n",
      "| Dest.ROA                  |              0.150009    |\n",
      "| Origin.SFO                |              0.147336    |\n",
      "| DayOfWeek.6               |              0.146539    |\n",
      "| DayOfWeek.4               |              0.145864    |\n",
      "| Dest.RSW                  |              0.143746    |\n",
      "| Origin.AVP                |              0.143028    |\n",
      "| Dest.SAN                  |              0.142381    |\n",
      "| Origin.RDU                |              0.138115    |\n",
      "| Dest.BWI                  |              0.135563    |\n",
      "| UniqueCarrier.DL          |              0.133596    |\n",
      "| Dest.PHX                  |              0.133555    |\n",
      "| Dest.MSP                  |              0.132945    |\n",
      "| Origin.SAT                |              0.1304      |\n",
      "| Origin.MDT                |              0.129945    |\n",
      "| Dest.GSP                  |              0.127479    |\n",
      "| Origin.MHT                |              0.127177    |\n",
      "| Month.1                   |              0.125886    |\n",
      "| Dest.OAK                  |              0.125856    |\n",
      "| Origin.LIT                |              0.123352    |\n",
      "| Origin.COS                |              0.12258     |\n",
      "| UniqueCarrier.UA          |              0.120839    |\n",
      "| Dest.TUL                  |              0.120256    |\n",
      "| Dest.RNO                  |              0.119465    |\n",
      "| Dest.DFW                  |              0.117803    |\n",
      "| Intercept                 |              0.115729    |\n",
      "| Dest.ORD                  |              0.114937    |\n",
      "| Dest.IAD                  |              0.111603    |\n",
      "| Origin.MSP                |              0.103788    |\n",
      "| Origin.SJC                |              0.102379    |\n",
      "| Origin.HOU                |              0.0990542   |\n",
      "| Dest.MDT                  |              0.0926851   |\n",
      "| Dest.MIA                  |              0.0909348   |\n",
      "| Origin.GEG                |              0.0902304   |\n",
      "| Dest.RDU                  |              0.0897475   |\n",
      "| Origin.SMF                |              0.0888477   |\n",
      "| Origin.PHX                |              0.0855419   |\n",
      "| Origin.SWF                |              0.0846649   |\n",
      "| DayOfWeek.7               |              0.0785089   |\n",
      "| Dest.PHF                  |              0.0773754   |\n",
      "| Origin.RIC                |              0.0713661   |\n",
      "| Dest.MAF                  |              0.0621032   |\n",
      "| Dest.ATL                  |              0.0606542   |\n",
      "| Year.1988                 |              0.0562897   |\n",
      "| Dest.HNL                  |              0.0541056   |\n",
      "| Dest.SYR                  |              0.0536571   |\n",
      "| Dest.ORF                  |              0.0529929   |\n",
      "| Origin.CHS                |              0.0514911   |\n",
      "| Origin.JFK                |              0.048292    |\n",
      "| Dest.PHL                  |              0.0455785   |\n",
      "| UniqueCarrier.PS          |              0.0440552   |\n",
      "| Origin.ONT                |              0.0391532   |\n",
      "| FlightNum                 |              0.0351439   |\n",
      "| Origin.DCA                |              0.0336946   |\n",
      "| Year.1989                 |              0.0320511   |\n",
      "| Origin.BNA                |              0.0299816   |\n",
      "| Dest.DEN                  |              0.0265254   |\n",
      "| Dest.SJC                  |              0.0241736   |\n",
      "| Dest.PIT                  |              0.0235111   |\n",
      "| DayOfWeek.3               |              0.0218137   |\n",
      "| Year.1998                 |              0.0215642   |\n",
      "| Dest.ABE                  |              0.0145978   |\n",
      "| Dest.LGA                  |              0.0140575   |\n",
      "| Dest.ILM                  |              0.0109377   |\n",
      "| Dest.HOU                  |              0.00748252  |\n",
      "| Origin.SNA                |              0.00161759  |\n",
      "| Dest.JAN                  |              0.00143587  |\n",
      "| Dest.DCA                  |              0.000720623 |\n",
      "| Origin.LAS                |              0.000677784 |\n",
      "| Dest.OKC                  |              0.000602466 |\n",
      "| Dest.ANC                  |              0           |\n",
      "| Dest.BGM                  |              0           |\n",
      "| Origin.ANC                |              0           |\n",
      "| Origin.MFR                |              0           |\n",
      "| Dest.SRQ                  |              0           |\n",
      "| Origin.SCK                |              0           |\n",
      "| Dest.SDF                  |              0           |\n",
      "| UniqueCarrier.missing(NA) |              0           |\n",
      "| Dest.SCK                  |              0           |\n",
      "| Dest.STT                  |              0           |\n",
      "| Dest.MSY                  |              0           |\n",
      "| Dest.LEX                  |              0           |\n",
      "| Dest.DSM                  |              0           |\n",
      "| Dest.BOI                  |              0           |\n",
      "| Origin.ABE                |              0           |\n",
      "| Dest.MDW                  |              0           |\n",
      "| Dest.DTW                  |              0           |\n",
      "| Origin.AMA                |              0           |\n",
      "| Dest.CVG                  |              0           |\n",
      "| Dest.ERI                  |              0           |\n",
      "| Dest.TUS                  |              0           |\n",
      "| Origin.TUL                |              0           |\n",
      "| Origin.BIL                |              0           |\n",
      "| Dest.GRR                  |              0           |\n",
      "| Month.missing(NA)         |              0           |\n",
      "| Dest.HPN                  |              0           |\n",
      "| Dest.EWR                  |              0           |\n",
      "| Origin.GRR                |              0           |\n",
      "| Origin.ABQ                |              0           |\n",
      "| Dest.DAL                  |              0           |\n",
      "| Origin.SJU                |              0           |\n",
      "| Dest.MRY                  |              0           |\n",
      "| Dest.TOL                  |              0           |\n",
      "| Origin.MAF                |              0           |\n",
      "| Dest.ORH                  |              0           |\n",
      "| Origin.missing(NA)        |              0           |\n",
      "| Dest.ACY                  |              0           |\n",
      "| Dest.CRP                  |              0           |\n",
      "| Dest.FNT                  |              0           |\n",
      "| Origin.ICT                |              0           |\n",
      "| Dest.missing(NA)          |              0           |\n",
      "| Origin.EWR                |              0           |\n",
      "| Dest.ELP                  |              0           |\n",
      "| Dest.CAK                  |              0           |\n",
      "| Dest.SJU                  |              0           |\n",
      "| Origin.STT                |              0           |\n",
      "| Origin.GNV                |              0           |\n",
      "| Dest.CHA                  |              0           |\n",
      "| Dest.ELM                  |              0           |\n",
      "| Origin.DTW                |              0           |\n",
      "| Origin.SBN                |              0           |\n",
      "| Dest.AMA                  |              0           |\n",
      "| Dest.BHM                  |              0           |\n",
      "| Origin.JAN                |              0           |\n",
      "| DayOfWeek.1               |              0           |\n",
      "| Origin.DSM                |              0           |\n",
      "| Origin.HRL                |              0           |\n",
      "| Dest.AVP                  |              0           |\n",
      "| Dest.EYW                  |              0           |\n",
      "| Dest.OGG                  |              0           |\n",
      "| Year.missing(NA)          |              0           |\n",
      "| DayOfWeek.missing(NA)     |              0           |\n",
      "| Origin.PDX                |              0           |\n",
      "| Origin.RSW                |              0           |\n",
      "| Dest.MYR                  |              0           |\n",
      "| Origin.KOA                |              0           |\n",
      "| Origin.BGM                |              0           |\n",
      "| Dest.EUG                  |              0           |\n",
      "| Origin.MEM                |              0           |\n",
      "| Origin.DAL                |              0           |\n",
      "| Origin.LAN                |              0           |\n",
      "| Dest.RIC                  |              0           |\n",
      "| Dest.PWM                  |              0           |\n",
      "| Dest.BOS                  |              0           |\n",
      "| Dest.LIT                  |              0           |\n"
     ]
    }
   ],
   "source": [
    "# Calculate magnitude of normalized GLM coefficients\n",
    "from six import iteritems\n",
    "glm_varimp = data_glm.coef_norm()\n",
    "for k,v in iteritems(glm_varimp):\n",
    "    glm_varimp[k] = abs(glm_varimp[k])\n",
    "    \n",
    "# Sort in descending order by magnitude\n",
    "glm_sorted = sorted(glm_varimp.items(), key = operator.itemgetter(1), reverse = True)\n",
    "table = tabulate(glm_sorted, headers = [\"Predictor\", \"Normalized Coefficient\"], tablefmt = \"orgtbl\")\n",
    "print(\"Coefficient Magnitudes:\\n\\n\" + table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xrange' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-7613547e1caf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoefficient_magnitudes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m'aqua'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"GLM Coefficient Magnitudes\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfontsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mxticks_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.65\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_width\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_xy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'xrange' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1162ae978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot GLM Coefficient Magnitudes\n",
    "all_coefficient_magnitudes = pandas.DataFrame(glm_sorted)\n",
    "coefficient_magnitudes = all_coefficient_magnitudes[1:10]\n",
    "feature_labels = list(coefficient_magnitudes[0])\n",
    "Index = coefficient_magnitudes.index\n",
    "\n",
    "plt.figure(figsize=(16,5))\n",
    "h = plt.bar(xrange(len(feature_labels)), coefficient_magnitudes[1],width=0.6, label=feature_labels, color ='aqua')\n",
    "plt.title(\"GLM Coefficient Magnitudes\", fontsize=20 )\n",
    "xticks_pos = [0.65*patch.get_width() + patch.get_xy()[0] for patch in h]\n",
    "plt.xticks(xticks_pos, feature_labels, fontsize=13,  ha='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# DRF Variable Importance\n",
    "data_rf2.varimp(use_pandas=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot DRF Feature Importances\n",
    "importances = data_rf2.varimp(use_pandas=True)\n",
    "feature_labels = list(importances['variable'])\n",
    "Index = importances.index\n",
    "\n",
    "plt.figure(figsize=(14,5))\n",
    "h = plt.bar(xrange(len(feature_labels)), importances['relative_importance'],width=0.6, label=feature_labels, color ='aqua')\n",
    "plt.title(\"DRF Feature Importances\", fontsize=20 )\n",
    "xticks_pos = [0.65*patch.get_width() + patch.get_xy()[0] for patch in h]\n",
    "plt.xticks(xticks_pos, feature_labels, fontsize=12,  ha='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# GBM Variable Importance\n",
    "data_gbm2.varimp(use_pandas=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot GBM Feature Importances\n",
    "importances = data_gbm2.varimp(use_pandas=True)\n",
    "feature_labels = list(importances['variable'])\n",
    "Index = importances.index\n",
    "\n",
    "plt.figure(figsize=(14,5))\n",
    "h = plt.bar(xrange(len(feature_labels)), importances['relative_importance'],width=0.6, label=feature_labels, color ='aqua')\n",
    "plt.title(\"GBM Feature Importances\", fontsize=20 )\n",
    "xticks_pos = [0.65*patch.get_width() + patch.get_xy()[0] for patch in h]\n",
    "plt.xticks(xticks_pos, feature_labels, fontsize=12,  ha='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Deep Learning Variable Importance\n",
    "data_dl.varimp(use_pandas=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot Deep Learning Feature Importances\n",
    "all_coefficient_magnitudes = data_dl.varimp(use_pandas=True)\n",
    "importances = all_coefficient_magnitudes[1:10]\n",
    "feature_labels = list(importances['variable'])\n",
    "Index = importances.index\n",
    "\n",
    "plt.figure(figsize=(20,6))\n",
    "h = plt.bar(xrange(len(feature_labels)), importances['relative_importance'],width=0.6, label=feature_labels, color ='aqua')\n",
    "plt.title(\"Deep Learning Feature Importances\",fontsize = 20)\n",
    "xticks_pos = [0.65*patch.get_width() + patch.get_xy()[0] for patch in h]\n",
    "plt.xticks(xticks_pos, feature_labels,fontsize = 13,   ha='right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"question\"></a>\n",
    "# Can H2O Handle New Categorical Levels in a Test Set? \n",
    "___\n",
    "> *Yes! Unlike most machine learning algorithms, H2O-3's algorithms can successfully make predictions, even if a test set contains categorical levels that were not present in the training set. This is because every algorithm handles new categorical levels specifically. So, the next question becomes:* \n",
    "\n",
    "> __How does each algorithm handle unseen categorical levels in a test set?__ \n",
    "\n",
    "### Skip to the algorithm you're using to see how it predicts on a categorical level not seen during training:<a name=\"answer\"></a>\n",
    "\n",
    "* [Generalized Linear Model (GLM)](#GLM)\n",
    "* [Gradient Boosting Model (GBM) & Distributed Random Forest (DRF)](#GBM & DRF)\n",
    "* [Deep Learning](#DL)\n",
    "* [K-Means](#K)\n",
    "* [Naive Bayes](#NB)\n",
    "* [Principal Component Analysis (PCA)](#PCA)\n",
    "\n",
    "\n",
    "___\n",
    "\n",
    "<a name=\"GLM\"></a>\n",
    " ** GLM **:\n",
    "\n",
    "  GLM will predict 'Double.NAN' for each row with a new categorical level, indicating a prediction wasn't made.\n",
    "  \n",
    "  *After running the cells to load, clean, and split the data you can play with a GLM [here](#GLM Model).*\n",
    "\n",
    "<a name=\"GBM & DRF\"></a>\n",
    " ** DRF & GBM ** :\n",
    "  \n",
    "  Unseen factors can go either left or right for small counts of factor levels. Otherwise, for large counts, they go\n",
    "  left. (To see a visual demonstration of tree splitting for large and small counts please see the [Missing Values & Unseen Categoricals GBM Demo](add link here)).\n",
    "\n",
    "  *After running the cells to load, clean, and split the data you can play with a GBM [here](#GBM Model) or a DRF [here](#DRF Model).*\n",
    "\n",
    "\n",
    "<a name=\"DL\"></a>\n",
    " ** Deep Learning **:\n",
    "\n",
    "  For an unseen categorical level in the test set, Deep Learning makes an extra input neuron that remains untrained and contributes some random amount to the subsequent layer.\n",
    "\n",
    "  *After running the cells to load, clean, and split the data you can play with a Deep Learning model [here](#DL Model).*\n",
    "\n",
    "<a name=\"K\"></a>\n",
    " ** K-Means **: \n",
    "\n",
    "  An unseen categorical level in a row does not contribute to that row's prediction. This is because the unseen  categorical level does not contribute to the distance comparison between clusters, and therefore does not factor in  predicting the cluster to which that row belongs.\n",
    "\n",
    "<a name=\"NB\"></a>\n",
    " ** Naive Bayes **:\n",
    " \n",
    "  If the Laplace smoothing parameter is disabled ('laplace = 0'), then Naive Bayes will predict a probability of 0 for any row in the test set that contains a previously unseen categorical level. However, if the Laplace smoothing parameter is used (e.g. 'laplace = 1'), then the model can make predictions for rows that include previously unseen categorical level.\n",
    "  \n",
    "  Laplace smoothing adjusts the maximum likelihood estimates by adding 1 to the numerator and k to the denominator to allow for new categorical levels in the training set:\n",
    "\n",
    "$$\\phi_{j|y=1}= \\frac{\\Sigma_{i=1}^m 1(x_{j}^{(i)} \\ = \\ 1 \\ \\bigcap y^{(i)} \\ = \\ 1) \\ + \\ 1}{\\Sigma_{i=1}^{m}1(y^{(i)} \\ = \\ 1) \\ + \\ k}$$\n",
    "\n",
    "$$\\phi_{j|y=0}= \\frac{\\Sigma_{i=1}^m 1(x_{j}^{(i)} \\ = \\ 1 \\ \\bigcap y^{(i)} \\ = \\ 0) \\ + \\ 1}{\\Sigma_{i \\ = \\ 1}^{m}1(y^{(i)} \\ = \\ 0) \\ + \\ k}$$\n",
    "   \n",
    "(Where $x^{(i)}$ represents features, $y^{(i)}$ represents the response column, and $k$ represents the addition of each new categorical level (k functions to balance the added 1 in the numerator))\n",
    "\n",
    "Laplace smoothing should be used with care; it is generally intended to allow for predictions in rare events. As prediction data becomes increasingly distinct from training data, new models should be trained when possible to account for a broader set of possible feature values.\n",
    "\n",
    "  *After running the cells to load, clean, and split the data you can play with a Naive Bayes model [here](#NB Model).*\n",
    "\n",
    "<a name=\"PCA\"></a>\n",
    " ** PCA **: \n",
    "\n",
    "  New categorical levels in the test data that were not present in the training data, are skipped in the row product-  sum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"missing\"></a>\n",
    "## How Does H2O Handle Missing Values during Training & Testing? \n",
    "\n",
    "### Skip to the algorithm you're using to see how it trains or predicts with missing values:\n",
    "\n",
    "(Note: NA values in the training set are not neccessarily handled the same way as NA values in the test set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "* [Generalized Linear Model (GLM)](#mGLM)\n",
    "* [Gradient Boosting Model (GBM) & Distributed Random Forest (DRF)](#mGBM & DRF)\n",
    "* [Deep Learning](#mDL)\n",
    "* [K-Means](#mK)\n",
    "* [Naive Bayes](#mNB)\n",
    "* [Principal Component Analysis (PCA)](#mPCA)\n",
    "\n",
    "\n",
    "___\n",
    "\n",
    "<a name=\"mGLM\"></a>\n",
    " ** GLM **:\n",
    "\n",
    "  *How does the algorithm handle missing values during training?*\n",
    "  \n",
    "  Depending on the selected missing value handling policy, they are either imputed mean or the whole row is skipped.\n",
    "  The default behavior is mean imputation. Note that categorical variables are imputed by adding extra \"missing\"\n",
    "  level.\n",
    "  \n",
    "  Optionally, glm can skip all rows with any missing values.\n",
    "  \n",
    "  *How does the algorithm handle missing values during testing?* \n",
    "  \n",
    "  Same as during training. If the missing value handling is set to skip and we are generating predictions, skipped\n",
    "  rows will have Na (missing) prediction.\n",
    "\n",
    "<a name=\"mGBM & DRF\"></a>\n",
    " ** DRF & GBM ** :\n",
    "\n",
    "\n",
    "  *How does the algorithm handle missing values during training and testing?*\n",
    "  \n",
    "  Missing values always go right at every split decision.\n",
    " \n",
    "  \n",
    "<a name=\"mDL\"></a>\n",
    " ** Deep Learning **:\n",
    " \n",
    "  *How does the algorithm handle missing values during training?*\n",
    "  \n",
    "  Missing values in the training set will be mean-imputed or the whole row can be skipped, depending on how the\n",
    "  following parameter is set: <code>missing_values_handling = \"MeanImputation\" or \"Skip\"</code>.\n",
    "\n",
    "   *How does the algorithm handle missing values during testing?*\n",
    "   \n",
    "   Missing values in the test set will be mean-imputed (with the mean of the training data) during scoring.\n",
    "\n",
    "\n",
    "<a name=\"mK\"></a>\n",
    " ** K-Means **: \n",
    "\n",
    "  *How does the algorithm handle missing values during training?*\n",
    "  \n",
    "  Missing values are automatically imputed by the column mean. K-means also handles missing values by assuming that\n",
    "  missing feature distance contributions are equal to the average of all other distance term contributions.\n",
    "  \n",
    "  *How does the algorithm handle missing values during testing?*\n",
    "  \n",
    "  Missing values are automatically imputed by the column mean of the training data.\n",
    "\n",
    "<a name=\"mNB\"></a>\n",
    " ** Naive Bayes **:\n",
    "  \n",
    "  *How does the algorithm handle missing values during training?*\n",
    "  \n",
    "  All rows with one or more missing values (either in the predictors or the response) will be skipped during model building.\n",
    "  \n",
    "  *How does the algorithm handle missing values during testing?*\n",
    "  \n",
    "  If a predictor is missing, it will be skipped when taking the product of conditional probabilities in calculating\n",
    "  the joint probability conditional on the response.\n",
    "\n",
    "<a name=\"mPCA\"></a>\n",
    " ** PCA **: \n",
    " \n",
    "  *How does the algorithm handle missing values during scoring?*\n",
    "    \n",
    "  For the GramSVD and Power methods, all rows containing missing values are ignored during training. For the GLRM\n",
    "  method, missing values are excluded from the sum over the loss function in the objective. For more information,\n",
    "  refer to section 4 Generalized Loss Functions, equation (13), in \"Generalized Low Rank Models\" by Boyd et al.\n",
    "  \n",
    "  *How does the algorithm handle missing values during testing?*\n",
    "  \n",
    "  During scoring, the test data is right-multiplied by the eigenvector matrix produced by PCA. Missing categorical\n",
    "  values are skipped in the row product-sum. Missing numeric values propagate an entire row of NAs in the resulting\n",
    "  projection matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"resources\"></a>\n",
    "## Additional Resources\n",
    "0. [H2O's How To for Basic Questions](http://h2oai.github.io/data-science-examples/examples.html)\n",
    "1. [H2O's User Guide Documentation](http://h2o-release.s3.amazonaws.com/h2o/rel-tibshirani/8/docs-website/h2o-docs/index.html)\n",
    "2. [H2O's Algorithm Documentation](https://github.com/h2oai/h2o-3/blob/master/h2o-docs/src/product/tutorials/datascience/DataScienceH2O-Dev.md)\n",
    "3. [H2O's Community - Where Questions are Posted](https://groups.google.com/forum/#!forum/h2ostream)\n",
    "4. [H2O's Python Demos](https://github.com/h2oai/h2o-3/tree/master/h2o-py/demos)\n",
    "5. [Python to R Parity](https://github.com/h2oai/h2o-3/blob/master/h2o-docs/src/product/upgrade/PythonParity.md)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
